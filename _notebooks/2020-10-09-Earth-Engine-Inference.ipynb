{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference in Google Earth Engine + Colab\n",
    "> Scaling up machine learning with GEE and Google Colab.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- author: Drew Bollinger\n",
    "- comments: false\n",
    "- hide: false\n",
    "- sticky_rank: 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SHAc5qbiR8l"
   },
   "source": [
    "# Inference in Google Earth Engine + Colab\n",
    "\n",
    "Here we demonstrate how to take a trained model and apply to to imagery with Google Earth Engine + Colab + Tensorflow. This is adapted from an [Earth Engine <> TensorFlow demonstration notebook](https://developers.google.com/earth-engine/guides/tf_examples). We'll be taking the trained model from the [Deep Learning Crop Type Segmentation Model Example](https://developmentseed.org/sat-ml-training/DeepLearning_CropType_Segmentation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MJ4kW1pEhwP"
   },
   "source": [
    "# Setup software libraries\n",
    "\n",
    "Authenticate and import as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jat01FEoUMqg"
   },
   "outputs": [],
   "source": [
    "# Import, authenticate and initialize the Earth Engine library.\n",
    "import ee\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpFD0Y_fETPL"
   },
   "outputs": [],
   "source": [
    "# Mount our Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RnZzcYhcpsQ"
   },
   "outputs": [],
   "source": [
    "# Add necessary libraries.\n",
    "!pip install -q focal-loss\n",
    "import os\n",
    "from os import path as op\n",
    "import tensorflow as tf\n",
    "import folium\n",
    "from focal_loss import SparseCategoricalFocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iT8ycmzClYwf"
   },
   "source": [
    "# Variables\n",
    "\n",
    "Declare the variables that will be in use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psz7wJKalaoj"
   },
   "outputs": [],
   "source": [
    "# Specify names locations for outputs in Google Drive. \n",
    "FOLDER = 'servir-inference-demo'\n",
    "ROOT_DIR = '/content/drive/My Drive/'\n",
    "\n",
    "# Specify inputs (Sentinel indexes) to the model.\n",
    "BANDS = ['NDVI', 'WDRVI', 'SAVI']\n",
    "\n",
    "# Specify the size and shape of patches expected by the model.\n",
    "KERNEL_SIZE = 224\n",
    "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgoDc7Hilfc4"
   },
   "source": [
    "# Imagery\n",
    "\n",
    "Gather and setup the imagery to use for inputs.  It's important that we match the index inputs from the earlier analysis. This is a three-month Sentinel-2 composite.  Display it in the notebook for a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IlgXu-vcUEY"
   },
   "outputs": [],
   "source": [
    "# Use Sentinel-2 data.\n",
    "\n",
    "def add_indexes(img): \n",
    "    ndvi = img.expression(\n",
    "        '(nir - red) / (nir  + red + a)', {\n",
    "            'a': 1e-5,\n",
    "            'nir': img.select('B8'),\n",
    "            'red': img.select('B4')\n",
    "        }\n",
    "       \n",
    "    ).rename('NDVI')\n",
    "\n",
    "    wdrvi = img.expression(\n",
    "        '(a * nir - red) / (a * nir + red)', {\n",
    "            'a': 0.2,\n",
    "            'nir': img.select('B8'),\n",
    "            'red': img.select('B4')\n",
    "        }\n",
    "    ).rename('WDRVI')\n",
    "\n",
    "    savi = img.expression(\n",
    "        '1.5 * (nir - red) / (nir + red + 0.5)', {\n",
    "            'nir': img.select('B8'),\n",
    "            'red': img.select('B4')\n",
    "        }\n",
    "    ).rename('SAVI')\n",
    "\n",
    "    return ee.Image.cat([ndvi, wdrvi, savi])\n",
    "\n",
    "image = ee.ImageCollection('COPERNICUS/S2') \\\n",
    "    .filterDate('2018-01-01', '2018-04-01') \\\n",
    "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \\\n",
    "    .map(add_indexes) \\\n",
    "    .median()\n",
    "\n",
    "# Use folium to visualize the imagery.\n",
    "mapid = image.getMapId({'bands': BANDS, 'min': -1, 'max': 1})\n",
    "map = folium.Map(location=[              \n",
    "              -29.177943749121233,\n",
    "              30.55984497070313,\n",
    "])\n",
    "folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
    "    overlay=True,\n",
    "    name='median composite',\n",
    "  ).add_to(map)\n",
    "\n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95dkWw-jLt5-"
   },
   "source": [
    "# Load our saved model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RJpNfEUS1qp"
   },
   "outputs": [],
   "source": [
    "# Load a trained model.\n",
    "MODEL_DIR = '/content/drive/Shared drives/servir-sat-ml/data/model_out/10062020/'\n",
    "model =  tf.keras.models.load_model(MODEL_DIR)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1ySNup0xCqN"
   },
   "source": [
    "# Prediction\n",
    "\n",
    "The prediction pipeline is:\n",
    "\n",
    "1.  Export imagery on which to do predictions from Earth Engine in TFRecord format to Google Drive.\n",
    "2.  Use the trained model to make the predictions.\n",
    "3.  Write the predictions to a TFRecord file in Google Drive.\n",
    "4.  Manually upload the predictions TFRecord file to Earth Engine.\n",
    "\n",
    "The following functions handle this process.  It's useful to separate the export from the predictions so that you can experiment with different models without running the export every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3WDAa-RUpXP"
   },
   "outputs": [],
   "source": [
    "def doExport(out_image_base, shape, region):\n",
    "  \"\"\"Run the image export task.  Block until complete.\n",
    "  \"\"\"\n",
    "  task = ee.batch.Export.image.toDrive(\n",
    "    image = image.select(BANDS),\n",
    "    description = out_image_base,\n",
    "    fileNamePrefix = out_image_base,\n",
    "    folder = FOLDER,\n",
    "    region = region.getInfo()['coordinates'],\n",
    "    scale = 30,\n",
    "    fileFormat = 'TFRecord',\n",
    "    maxPixels = 1e10,\n",
    "    formatOptions = {\n",
    "      'patchDimensions': shape,\n",
    "      'compressed': True,\n",
    "      'maxFileSize': 104857600\n",
    "    }\n",
    "  )\n",
    "  task.start()\n",
    "\n",
    "  # Block until the task completes.\n",
    "  print('Running image export to Google Drive...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)\n",
    "\n",
    "  # Error condition\n",
    "  if task.status()['state'] != 'COMPLETED':\n",
    "    print('Error with image export.')\n",
    "  else:\n",
    "    print('Image export completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zb_9_FflygVw"
   },
   "outputs": [],
   "source": [
    "def doPrediction(out_image_base, kernel_shape, region):\n",
    "  \"\"\"Perform inference on exported imagery.\n",
    "  \"\"\"\n",
    "\n",
    "  print('Looking for TFRecord files...')\n",
    "\n",
    "  # Get a list of all the files in the output bucket.\n",
    "  filesList = os.listdir(op.join(ROOT_DIR, FOLDER))\n",
    "\n",
    "  # Get only the files generated by the image export.\n",
    "  exportFilesList = [s for s in filesList if out_image_base in s]\n",
    "\n",
    "  # Get the list of image files and the JSON mixer file.\n",
    "  imageFilesList = []\n",
    "  jsonFile = None\n",
    "  for f in exportFilesList:\n",
    "    if f.endswith('.tfrecord.gz'):\n",
    "      imageFilesList.append(op.join(ROOT_DIR, FOLDER, f))\n",
    "    elif f.endswith('.json'):\n",
    "      jsonFile = f\n",
    "\n",
    "  # Make sure the files are in the right order.\n",
    "  imageFilesList.sort()\n",
    "\n",
    "  from pprint import pprint\n",
    "  pprint(imageFilesList)\n",
    "  print(jsonFile)\n",
    "\n",
    "  import json\n",
    "  # Load the contents of the mixer file to a JSON object.\n",
    "  with open(op.join(ROOT_DIR, FOLDER, jsonFile), 'r') as f:\n",
    "    mixer = json.load(f)\n",
    "\n",
    "  pprint(mixer)\n",
    "  patches = mixer['totalPatches']\n",
    "\n",
    "  # Get set up for prediction.\n",
    "\n",
    "  imageColumns = [\n",
    "    tf.io.FixedLenFeature(shape=kernel_shape, dtype=tf.float32) \n",
    "      for k in BANDS\n",
    "  ]\n",
    "\n",
    "  imageFeaturesDict = dict(zip(BANDS, imageColumns))\n",
    "\n",
    "  def parse_image(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, imageFeaturesDict)\n",
    "\n",
    "  def toTupleImage(inputs):\n",
    "    inputsList = [inputs.get(key) for key in BANDS]\n",
    "    stacked = tf.stack(inputsList, axis=0)\n",
    "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
    "    return stacked\n",
    "\n",
    "   # Create a dataset from the TFRecord file(s) in Cloud Storage.\n",
    "  imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
    "  imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)\n",
    "  imageDataset = imageDataset.map(toTupleImage).batch(1)\n",
    "\n",
    "  # Perform inference.\n",
    "  print('Running predictions...')\n",
    "  predictions = model.predict(imageDataset, steps=patches, verbose=1)\n",
    "  # print(predictions[0])\n",
    "\n",
    "  print('Writing predictions...')\n",
    "  out_image_file = op.join(ROOT_DIR, FOLDER, f'{out_image_base}pred.TFRecord')\n",
    "  writer = tf.io.TFRecordWriter(out_image_file)\n",
    "  patches = 0\n",
    "  for predictionPatch in predictions:\n",
    "    print('Writing patch ' + str(patches) + '...')\n",
    "    predictionPatch = tf.argmax(predictionPatch, axis=2)\n",
    "\n",
    "    # Create an example.\n",
    "    example = tf.train.Example(\n",
    "      features=tf.train.Features(\n",
    "        feature={\n",
    "          'class': tf.train.Feature(\n",
    "              float_list=tf.train.FloatList(\n",
    "                  value=predictionPatch.numpy().flatten()))\n",
    "        }\n",
    "      )\n",
    "    )\n",
    "    # Write the example.\n",
    "    writer.write(example.SerializeToString())\n",
    "    patches += 1\n",
    "\n",
    "  writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZqlymOehnQO"
   },
   "source": [
    "Now there's all the code needed to run the prediction pipeline, all that remains is to specify the output region in which to do the prediction, the names of the output files, where to put them, and the shape of the outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPANwc7B1-TS"
   },
   "outputs": [],
   "source": [
    "# Base file name to use for TFRecord files and assets.\n",
    "image_base = 'servir_inference_demo_'\n",
    "\n",
    "# South Africa (near training data)\n",
    "region = ee.Geometry.Polygon(\n",
    "        [[[\n",
    "              30.55984497070313,\n",
    "              -29.177943749121233\n",
    "            ],\n",
    "            [\n",
    "              30.843429565429684,\n",
    "              -29.177943749121233\n",
    "            ],\n",
    "            [\n",
    "              30.843429565429684,\n",
    "              -28.994928377910732\n",
    "            ],\n",
    "            [\n",
    "              30.55984497070313,\n",
    "              -28.994928377910732\n",
    "            ]]], None, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLNEOLkXWvSi"
   },
   "outputs": [],
   "source": [
    "# Run the export.\n",
    "doExport(image_base, KERNEL_SHAPE, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KxACnxKFrQ_J"
   },
   "outputs": [],
   "source": [
    "# Run the prediction.\n",
    "doPrediction(image_base, KERNEL_SHAPE, region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uj_G9OZ1xH6K"
   },
   "source": [
    "# Display the output\n",
    "\n",
    "One the data has been exported, the model has made predictions and the predictions have been written to a file, we need to [manually import the TFRecord to Earth Engine](https://developers.google.com/earth-engine/guides/tfrecord#uploading-tfrecords-to-earth-engine). Then we can display our crop type predictions as an image asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jgco6HJ4R5p2"
   },
   "outputs": [],
   "source": [
    "out_image = ee.Image('users/drew/servir_inference_demo_-mixer')\n",
    "mapid = out_image.getMapId({'min': 0, 'max': 10, 'palette': ['00A600','63C600','E6E600','E9BD3A','ECB176','EFC2B3','F2F2F2']})\n",
    "map = folium.Map(location=[              \n",
    "              -29.177943749121233,\n",
    "              30.55984497070313,\n",
    "])\n",
    "folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
    "    overlay=True,\n",
    "    name='predicted crop type',\n",
    "  ).add_to(map)\n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "UNET_regression_demo.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
