{
  
    
        "post0": {
            "title": "Working at Scale",
            "content": "Working With STAC - At Scale . STAC: SpatioTemporal Asset Catalog . The SpatioTemporal Asset Catalog (STAC) specification aims to standardize the way geospatial assets are exposed online and queried. A &#39;spatiotemporal asset&#39; is any file that represents information about the earth captured in a certain space and time. The initial focus is primarily remotely-sensed imagery (from satellites, but also planes, drones, balloons, etc), but the core is designed to be extensible to SAR, full motion video, point clouds, hyperspectral, LiDAR and derived data like NDVI, Digital Elevation Models, mosaics, etc. . Ref:https://github.com/radiantearth/stac-spechttps://github.com/radiantearth/stac-spec Using STAC makes data indexation and discovery really easy. In addition to the Collection/Item/Asset (data) specifications, data providers are also encouraged to follow a STAC API specification: https://github.com/radiantearth/stac-api-spec . The API is compliant with the OGC API - Features standard (formerly known as OGC Web Feature Service 3), in that it defines many of the endpoints that STAC uses. A STAC API should be compatible and usable with any OGC API - Features clients. The STAC API can be thought of as a specialized Features API to search STAC Catalogs, where the features returned are STAC Items, that have common properties, links to their assets and geometries that represent the footprints of the geospatial assets. . Sentinel 2 . Thanks to Digital Earth Africa and in collaboration with Sinergise, Element 84, Amazon Web Services (AWS) and the Committee on Earth Observation Satellites (CEOS), Sentinel 2 (Level 2) data over Africa, usually stored as JPEG2000, has been translated to COG more important a STAC database and API has been setup. . https://www.digitalearthafrica.org/news/operational-and-ready-use-satellite-data-now-available-across-africa The API is provided by @element84 and follows the latest specification: https://earth-search.aws.element84.com/v0 . TiTiler: STAC + COG . Docs: https://github.com/developmentseed/titiler/blob/master/docs/endpoints/stac.md . TiTiler was first designed to work with single COG by passing the file URL to the tiler. e.g : https://myendpoint/cog/tiles/1/2/3?url=https://somewhere.com/mycog.tif . With STAC is a bit different because we first have to read the STAC items and then know which assets to read. . Example of STAC Item . { &quot;type&quot;: &quot;Feature&quot;, &quot;id&quot;: &quot;S2A_34SGA_20200318_0_L2A&quot;, &quot;geometry&quot;: {...}, &quot;properties&quot;: { &quot;datetime&quot;: &quot;2020-03-18T09:11:33Z&quot;, ... }, &quot;collection&quot;: &quot;sentinel-s2-l2a-cogs&quot;, &quot;assets&quot;: { &quot;thumbnail&quot;: { &quot;title&quot;: &quot;Thumbnail&quot;, &quot;type&quot;: &quot;image/png&quot;, &quot;href&quot;: &quot;https://myurl.com/preview.jpg&quot; }, ... &quot;B03&quot;: { &quot;title&quot;: &quot;Band 3 (green)&quot;, &quot;type&quot;: &quot;image/tiff; application=geotiff; profile=cloud-optimized&quot;, &quot;href&quot;: &quot;https://myurl.com/B03.tif&quot;, &quot;proj:shape&quot;: [ 10980, 10980 ], &quot;proj:transform&quot;: [ 10, 0, 699960, 0, -10, 3600000, 0,-* 0, 1 ] }, ... }, &quot;links&quot;: [...] } . To be able to create Web Map tile from the B03 asset you&#39;ll need to pass the STAC Item url and the asset name: . https://myendpoint/stac/tiles/1/2/3?url=https://somewhere.com/item.json&amp;assets=B03 . Requirements . To be able to run this notebook you&#39;ll need the following requirements: . rasterio | ipyleaflet | requests | tqdm | . !pip install rasterio ipyleaflet requests tqdm . import os import json import base64 import requests import datetime import itertools import urllib.parse from io import BytesIO from functools import partial from concurrent import futures from tqdm.notebook import tqdm from rasterio.plot import reshape_as_image from rasterio.features import bounds as featureBounds from ipyleaflet import Map, basemaps, TileLayer, basemap_to_tiles, GeoJSON %pylab inline . Populating the interactive namespace from numpy and matplotlib . # Endpoint variables titiler_endpoint = &quot;https://api.cogeo.xyz/&quot; # Devseed temporary endpoint stac_endpoint = &quot;https://earth-search.aws.element84.com/v0/search&quot; . Search for STAC Items . See https://github.com/radiantearth/stac-api-spec for more documentation about the stac API . AOI | You can use geojson.io to define your search AOI . geojson = { &quot;type&quot;: &quot;FeatureCollection&quot;, &quot;features&quot;: [ { &quot;type&quot;: &quot;Feature&quot;, &quot;properties&quot;: {}, &quot;geometry&quot;: { &quot;type&quot;: &quot;Polygon&quot;, &quot;coordinates&quot;: [ [ [ 30.810813903808594, 29.454247067148533 ], [ 30.88600158691406, 29.454247067148533 ], [ 30.88600158691406, 29.51879923863822 ], [ 30.810813903808594, 29.51879923863822 ], [ 30.810813903808594, 29.454247067148533 ] ] ] } } ] } bounds = featureBounds(geojson) m = Map( basemap=basemaps.OpenStreetMap.Mapnik, center=((bounds[1] + bounds[3]) / 2,(bounds[0] + bounds[2]) / 2), zoom=11 ) geo_json = GeoJSON(data=geojson) m.add_layer(geo_json) m . Define dates and other filters | start = datetime.datetime.strptime(&quot;2019-01-01&quot;, &quot;%Y-%m-%d&quot;).strftime(&quot;%Y-%m-%dT00:00:00Z&quot;) end = datetime.datetime.strptime(&quot;2019-12-11&quot;, &quot;%Y-%m-%d&quot;).strftime(&quot;%Y-%m-%dT23:59:59Z&quot;) # POST body query = { &quot;collections&quot;: [&quot;sentinel-s2-l2a-cogs&quot;], &quot;datetime&quot;: f&quot;{start}/{end}&quot;, &quot;query&quot;: { &quot;eo:cloud_cover&quot;: { &quot;lt&quot;: 5 } }, &quot;intersects&quot;: geojson[&quot;features&quot;][0][&quot;geometry&quot;], &quot;limit&quot;: 100, &quot;fields&quot;: { &#39;include&#39;: [&#39;id&#39;, &#39;properties.datetime&#39;, &#39;properties.eo:cloud_cover&#39;], # This will limit the size of returned body &#39;exclude&#39;: [&#39;assets&#39;, &#39;links&#39;] # This will limit the size of returned body } } # POST Headers headers = { &quot;Content-Type&quot;: &quot;application/json&quot;, &quot;Accept-Encoding&quot;: &quot;gzip&quot;, &quot;Accept&quot;: &quot;application/geo+json&quot;, } data = requests.post(stac_endpoint, headers=headers, json=query).json() print(&quot;Results context:&quot;) print(data[&quot;context&quot;]) print() print(&quot;Example of item:&quot;) print(json.dumps(data[&quot;features&quot;][0], indent=4)) sceneid = [f[&quot;id&quot;] for f in data[&quot;features&quot;]] cloudcover = [f[&quot;properties&quot;][&quot;eo:cloud_cover&quot;] for f in data[&quot;features&quot;]] dates = [f[&quot;properties&quot;][&quot;datetime&quot;][0:10] for f in data[&quot;features&quot;]] . Results context: {&#39;page&#39;: 1, &#39;limit&#39;: 100, &#39;matched&#39;: 84, &#39;returned&#39;: 84} Example of item: { &#34;bbox&#34;: [ 30.155974613579858, 28.80949327971016, 31.050481437029678, 29.815791988006527 ], &#34;geometry&#34;: { &#34;coordinates&#34;: [ [ [ 30.155974613579858, 28.80949327971016 ], [ 30.407037927198104, 29.805008695373978 ], [ 31.031551610920825, 29.815791988006527 ], [ 31.050481437029678, 28.825387639743422 ], [ 30.155974613579858, 28.80949327971016 ] ] ], &#34;type&#34;: &#34;Polygon&#34; }, &#34;id&#34;: &#34;S2B_36RTT_20191205_0_L2A&#34;, &#34;collection&#34;: &#34;sentinel-s2-l2a-cogs&#34;, &#34;type&#34;: &#34;Feature&#34;, &#34;properties&#34;: { &#34;datetime&#34;: &#34;2019-12-05T08:42:04Z&#34;, &#34;eo:cloud_cover&#34;: 2.75 } } . m = Map( basemap=basemaps.OpenStreetMap.Mapnik, center=((bounds[1] + bounds[3]) / 2,(bounds[0] + bounds[2]) / 2), zoom=8 ) geo_json = GeoJSON( data=data, style={ &#39;opacity&#39;: 1, &#39;dashArray&#39;: &#39;1&#39;, &#39;fillOpacity&#39;: 0, &#39;weight&#39;: 1 }, ) m.add_layer(geo_json) m . Plot Date / Cloud Cover . fig = plt.figure(dpi=100) fig.autofmt_xdate() ax = fig.add_subplot(1, 1, 1) ax.plot(dates, cloudcover, label=&quot;Cloud Cover&quot;, color=&quot;tab:red&quot;, linewidth=0.4, linestyle=&quot;-.&quot;) ax.legend() . &lt;matplotlib.legend.Legend at 0x120997ed0&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Use Titiler endpoint . https://api.cogeo.xyz/docs#/SpatioTemporal%20Asset%20Catalog . {endpoint}/stac/tiles/{z}/{x}/{y}.{format}?url={stac_item}&amp;{otherquery params} . {endpoint}/stac/crop/{minx},{miny},{maxx},{maxy}.{format}?url={stac_item}&amp;{otherquery params} . {endpoint}/stac/point/{minx},{miny}?url={stac_item}&amp;{otherquery params} . url_template = &quot;https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/{id}&quot; . Visualize One Item . # Get Tile URL r = requests.get( f&quot;{titiler_endpoint}/stac/tilejson.json&quot;, params = { &quot;url&quot;: url_template.format(id=sceneid[0]), &quot;assets&quot;: &quot;B04,B03,B02&quot;, # Simple RGB combination (True Color) &quot;color_formula&quot;: &quot;Gamma RGB 3.5 Saturation 1.7 Sigmoidal RGB 15 0.35&quot;, # We use a rio-color formula to make the tiles look nice &quot;minzoom&quot;: 8, # By default titiler will use 0 &quot;maxzoom&quot;: 14, # By default titiler will use 24 } ).json() print(r) m = Map( center=((bounds[1] + bounds[3]) / 2,(bounds[0] + bounds[2]) / 2), zoom=10 ) tiles = TileLayer( url=r[&quot;tiles&quot;][0], min_zoom=r[&quot;minzoom&quot;], max_zoom=r[&quot;maxzoom&quot;], opacity=1 ) m.add_layer(tiles) m . {&#39;tilejson&#39;: &#39;2.2.0&#39;, &#39;name&#39;: &#39;S2B_36RTT_20191205_0_L2A&#39;, &#39;version&#39;: &#39;1.0.0&#39;, &#39;scheme&#39;: &#39;xyz&#39;, &#39;tiles&#39;: [&#39;https://api.cogeo.xyz/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?url=https%3A%2F%2Fearth-search.aws.element84.com%2Fv0%2Fcollections%2Fsentinel-s2-l2a-cogs%2Fitems%2FS2B_36RTT_20191205_0_L2A&amp;assets=B04%2CB03%2CB02&amp;color_formula=Gamma+RGB+3.5+Saturation+1.7+Sigmoidal+RGB+15+0.35&#39;], &#39;minzoom&#39;: 8, &#39;maxzoom&#39;: 14, &#39;bounds&#39;: [30.155974613579858, 28.80949327971016, 31.050481437029678, 29.815791988006527], &#39;center&#39;: [30.603228025304766, 29.312642633858346, 8]} . r = requests.get( f&quot;{titiler_endpoint}/stac/tilejson.json&quot;, params = { &quot;url&quot;: url_template.format(id=sceneid[0]), &quot;assets&quot;: &quot;B08,B04,B03&quot;, # False Color Infrared &quot;color_formula&quot;: &quot;Gamma RGB 3.5 Saturation 1.7 Sigmoidal RGB 15 0.35&quot;, &quot;minzoom&quot;: 8, # By default titiler will use 0 &quot;maxzoom&quot;: 14, # By default titiler will use 24 } ).json() print(r) m = Map( center=((bounds[1] + bounds[3]) / 2,(bounds[0] + bounds[2]) / 2), zoom=10 ) tiles = TileLayer( url=r[&quot;tiles&quot;][0], min_zoom=r[&quot;minzoom&quot;], max_zoom=r[&quot;maxzoom&quot;], opacity=1 ) m.add_layer(tiles) m . {&#39;tilejson&#39;: &#39;2.2.0&#39;, &#39;name&#39;: &#39;S2B_36RTT_20191205_0_L2A&#39;, &#39;version&#39;: &#39;1.0.0&#39;, &#39;scheme&#39;: &#39;xyz&#39;, &#39;tiles&#39;: [&#39;https://api.cogeo.xyz/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?url=https%3A%2F%2Fearth-search.aws.element84.com%2Fv0%2Fcollections%2Fsentinel-s2-l2a-cogs%2Fitems%2FS2B_36RTT_20191205_0_L2A&amp;assets=B08%2CB04%2CB03&amp;color_formula=Gamma+RGB+3.5+Saturation+1.7+Sigmoidal+RGB+15+0.35&#39;], &#39;minzoom&#39;: 8, &#39;maxzoom&#39;: 14, &#39;bounds&#39;: [30.155974613579858, 28.80949327971016, 31.050481437029678, 29.815791988006527], &#39;center&#39;: [30.603228025304766, 29.312642633858346, 8]} . r = requests.get( f&quot;{titiler_endpoint}/stac/tilejson.json&quot;, params = { &quot;url&quot;: url_template.format(id=sceneid[0]), &quot;expression&quot;: &quot;(B08-B04)/(B08+B04)&quot;, # NDVI &quot;rescale&quot;: &quot;-1,1&quot;, &quot;minzoom&quot;: 8, # By default titiler will use 0 &quot;maxzoom&quot;: 14, # By default titiler will use 24 &quot;color_map&quot;: &quot;viridis&quot;, } ).json() print(r) m = Map( center=((bounds[1] + bounds[3]) / 2,(bounds[0] + bounds[2]) / 2), zoom=10 ) tiles = TileLayer( url=r[&quot;tiles&quot;][0], min_zoom=r[&quot;minzoom&quot;], max_zoom=r[&quot;maxzoom&quot;], opacity=1 ) m.add_layer(tiles) m . {&#39;tilejson&#39;: &#39;2.2.0&#39;, &#39;name&#39;: &#39;S2B_36RTT_20191205_0_L2A&#39;, &#39;version&#39;: &#39;1.0.0&#39;, &#39;scheme&#39;: &#39;xyz&#39;, &#39;tiles&#39;: [&#39;https://api.cogeo.xyz/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?url=https%3A%2F%2Fearth-search.aws.element84.com%2Fv0%2Fcollections%2Fsentinel-s2-l2a-cogs%2Fitems%2FS2B_36RTT_20191205_0_L2A&amp;expression=%28B08-B04%29%2F%28B08%2BB04%29&amp;rescale=-1%2C1&amp;color_map=viridis&#39;], &#39;minzoom&#39;: 8, &#39;maxzoom&#39;: 14, &#39;bounds&#39;: [30.155974613579858, 28.80949327971016, 31.050481437029678, 29.815791988006527], &#39;center&#39;: [30.603228025304766, 29.312642633858346, 8]} . More . titiler doesn&#39;t return only png or jpeg but can also return Numpy array directly . def fetch_bbox_array(sceneid, bbox, assets = None, expression = None, **kwargs): &quot;&quot;&quot;Helper function to fetch and decode Numpy array using Titiler endpoint.&quot;&quot;&quot; # STAC ITEM URL stac_item = f&quot;https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/{sceneid}&quot; xmin, ymin, xmax, ymax = bbox # TiTiler required URL + asset or expression parameters params = {&quot;url&quot;: stac_item} if assets: params.update(dict(assets=&quot;,&quot;.join(assets))) elif expression: params.update(dict(expression=expression)) else: raise Exception(&quot;Missing band or expression input&quot;) params.update(**kwargs) # TITILER ENDPOINT url = f&quot;{titiler_endpoint}stac/crop/{xmin},{ymin},{xmax},{ymax}.npy&quot; r = requests.get(url, params=params) data, mask = numpy.load(BytesIO(r.content), allow_pickle=True) return sceneid, data, mask def _filter_futures(tasks): for future in tasks: try: yield future.result() except Exception: pass def _stats(data, mask): arr = numpy.ma.array(data) arr.mask = mask == 0 return arr.min().item(), arr.max().item(), arr.mean().item(), arr.std().item() . # Fetch one data _, data, mask = fetch_bbox_array(sceneid[0], bounds, assets=[&quot;B02&quot;], width=128, height=128) print(data.shape) print(mask.shape) imshow(data[0]) . (1, 128, 128) (128, 128) . &lt;matplotlib.image.AxesImage at 0x1237baa10&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; # Let&#39;s fetch the data over our AOI for all our Items # Here we use `futures.ThreadPoolExecutor` to run the requests in parallel # Note: it takes more time for the notebook to display the results than to fetch the data bbox_worker = partial( fetch_bbox_array, bbox=bounds, assets=(&quot;B04&quot;, &quot;B03&quot;, &quot;B02&quot;), color_formula=&quot;gamma RGB 3.5, saturation 1.7, sigmoidal RGB 15 0.35&quot;, width=64, height=64, ) with futures.ThreadPoolExecutor(max_workers=10) as executor: future_work = [ executor.submit(bbox_worker, scene) for scene in sceneid ] for f in tqdm(futures.as_completed(future_work), total=len(future_work)): pass results_rgb = list(_filter_futures(future_work)) print(&quot;diplay all results&quot;) fig = plt.figure(figsize=(10,20)) col = 5 row = math.ceil(len(dates) / col) for i in range(1, len(results_rgb) + 1): fig.add_subplot(row, col, i) plt.imshow(reshape_as_image(results_rgb[i-1][1])) . diplay all results . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; ## Fetch NDVI bbox_worker = partial( fetch_bbox_array, bbox=bounds, expression=&quot;(B08-B04)/(B08+B04)&quot;, width=64, height=64, ) with futures.ThreadPoolExecutor(max_workers=10) as executor: future_work = [ executor.submit(bbox_worker, scene) for scene in sceneid ] for f in tqdm(futures.as_completed(future_work), total=len(future_work)): pass results_ndvi = list(_filter_futures(future_work)) fig = plt.figure(figsize=(10,20)) col = 5 row = math.ceil(len(dates) / col) for i in range(1, len(results_rgb) + 1): fig.add_subplot(row, col, i) plt.imshow(results_ndvi[i-1][1][0]) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; stats = [_stats(data, mask) for _, data, mask in results_ndvi] fig, ax1 = plt.subplots(dpi=150) fig.autofmt_xdate() ax1.plot(dates, [s[0] for s in stats], label=&quot;Min&quot;) ax1.plot(dates, [s[1] for s in stats], label=&quot;Max&quot;) ax1.plot(dates, [s[2] for s in stats], label=&quot;Mean&quot;) ax1.set_xlabel(&quot;Dates&quot;) ax1.set_ylabel(&quot;Normalized Difference Vegetation Index&quot;) ax1.legend() . &lt;matplotlib.legend.Legend at 0x123da7650&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt;",
            "url": "https://developmentseed.github.io/sat-ml-training/Scaling",
            "relUrl": "/Scaling",
            "date": " • Oct 8, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Getting Started",
            "content": "This tutorial is all about getting everything setup so you can do the other tutorials on this site. In the process we will be reviewing fundamental python skills required. If at any point you&#39;re not sure what is happening please have a look at our background pages on Python, Geospatial Python, and Machine Learning. . Accounts . Google . For the lessons on this site you will need a Google Account (this can be @gmail or an institution/custom google account). You will use this account to access Google Drive, Google Colab, and activate Google Earth Engine. . Google Earth Engine . You will also need an account for Google Earth Engine for some lessons. You can sign up here, using your Google Account from the previous step to link everything together nicely. . Zindi . For some lessons we use data from Zindi.africa a Machine Learning competition site. Sign up here. . Connections . Let&#39;s go ahead an test some of our accounts . Google Drive . You will run the next code block. The output will show a link you have to open, copy the code from the page that loads and paste back into a cell provided. . from google.colab import drive drive.mount(&#39;/content/drive/&#39;) . Mounted at /content/drive/ . Explore your drive . Now that you&#39;ve connected (mounted) google drive to your session, you can access it as a local disk. . Look at the panel on the left, for the folder icon (3rd from the top). You should see your google drive under drive where you should see My Drive and Shared Drives. At any point if you hover over a folder three vertical dots appear on the right, and if you click you will see the option to copy path. . Let&#39;s now explore using python to list files. . import os # list the folders and files in your drive mydrive = &#39;/content/drive/My Drive&#39; colab = &#39;/content/drive/My Drive/Colab Notebooks&#39; print(os.listdir(mydrive)) # list the current working directory print(os.getcwd()) # make a data folder in your Colab Notebook directory if it doesn&#39;t exist data_folder = os.path.join(colab,&#39;data&#39;) os.path.isdir(data_folder) if (not os.path.isdir(data_folder)): os.mkdir(data_folder) # list the contents print(os.listdir(colab)) . [&#39;Getting started.pdf&#39;, &#39;projects&#39;, &#39;personal&#39;, &#39;labs&#39;, &#39;publications&#39;, &#39;proposals&#39;, &#39;team&#39;, &#39;PROJECT End-of-Sprint Demos Sprint Planning Agenda.gdoc&#39;, &#39;QGIS_US_donation_slide.pptx&#39;, &#39;community&#39;, &#39;Colab Notebooks&#39;] /content [&#39;EE_TF_PointData_Conv.ipynb&#39;, &#39;Copy of ee_ImageCollection.ipynb&#39;, &#39;data&#39;, &#39;.ipynb_checkpoints&#39;, &#39;randomforest.ipynb&#39;, &#39;data1&#39;, &#39;2020-02-19-GettingStarted.ipynb&#39;] . Google Earth Engine . Initialize . Next you will need to install the Google Earth Engine python API. If running on Colab it&#39;s already installed. It will autodetect and skip if already installed. . Then you need to initialize the Earth Engine session. The output will show a link you have to open, copy the code from the page that loads and paste back into a cell provided. . # If not on Colab you&#39;ll need install the earth-engine Python API #!pip install earthengine-api #earth-engine Python API # Athenticate to your GEE account. !earthengine authenticate . # Earth Engine Python API import ee ee.Initialize() . Do a search . Let&#39;s query Earth Engine [data collections] by looking for Imagery of Nepal. . We are going to: . Define a bounding box or center point | Identify the collection | Query and create an ImageCollection | Display on a map | Export | Reload Raster and Explore | Define a Bounding Box . You can define a Rectangle, Point, or Polygon for a spatial search by typing numbers, but that&#39;s tedious. It&#39;s much easier to get it from an existing data set or by selecting on a map. Let&#39;s get a Bounding Box in geojson format from geojson.io . Search for a place (eyeglass in upper right), e.g Kathmandu | Copy and Paste the geojson from the right panel, or Save GeoJSON file and then upload it to your Google Drive. . Note: Due to some limitations we can&#8217;t use ipyleaflet in Colab so we needed to use an external tool. | # Make sure we have the libraries we need %pip install folium %pip install geopandas . # Let see it on a map import folium import geopandas as gpd # Paste the Geojson from geojsonio aoi_geojson = &#39;&#39;&#39;{ &quot;type&quot;: &quot;FeatureCollection&quot;, &quot;features&quot;: [ { &quot;type&quot;: &quot;Feature&quot;, &quot;properties&quot;: {}, &quot;geometry&quot;: { &quot;type&quot;: &quot;Polygon&quot;, &quot;coordinates&quot;: [ [ [ 85.26712417602539, 27.66558933380944 ], [ 85.38213729858398, 27.66558933380944 ], [ 85.38213729858398, 27.747202035778272 ], [ 85.26712417602539, 27.747202035778272 ], [ 85.26712417602539, 27.66558933380944 ] ] ] } } ] }&#39;&#39;&#39; # Now let&#39;s read the geojson into a GeoPandasDataframe aoi = gpd.read_file(aoi_geojson) print(aoi) # so we can see what it looks like #Get the bounding box bbox = aoi.total_bounds print(bbox) # see the coordinates #Make it a GEE rectangle gee_aoi = ee.Geometry.Rectangle(bbox.tolist()) center = aoi.centroid[0] . geometry 0 POLYGON ((85.26712 27.66559, 85.38214 27.66559... [85.26712418 27.66558933 85.3821373 27.74720204] . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: UserWarning: Geometry is in a geographic CRS. Results from &#39;centroid&#39; are likely incorrect. Use &#39;GeoSeries.to_crs()&#39; to re-project geometries to a projected CRS before this operation. . Make this Notebook Trusted to load map: File -&gt; Trust Notebook&lt;iframe src=&quot;about:blank&quot; style=&quot;position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;&quot; data-html=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 onload=&quot;this.contentDocument.open();this.contentDocument.write(atob(this.getAttribute(&#39;data-html&#39;)));this.contentDocument.close();&quot; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt;&lt;/iframe&gt; m = folium.Map(location=[center.y, center.x], tiles=&quot;OpenStreetMap&quot;, zoom_start=12) folium.Marker( location=[center.y, center.x], popup=&#39;Center&#39;, icon=folium.Icon(color=&#39;green&#39;, icon=&#39;ok-sign&#39;), ).add_to(m) folium.features.GeoJson(aoi_geojson, style_function = lambda x: {&#39;color&#39;:&#39;blue&#39;, &#39;fillcolor&#39;:&#39;transparent&#39;} ).add_to(m) m . Query Google Earth Engine . Now lets pick a collection of imagery to query for the region. We can choose from anything in the Google Earth Engine data catalog . Let&#39;s start with the Landsat 8 Surface Reflectance this year, using a cloud mask and selecting the bands to make an R,G,B mosiac . # To make a map we first need some helper functions # Define the URL format used for Earth Engine generated map tiles. EE_TILES = &#39;https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}&#39; #@title Mapdisplay: Display GEE objects using folium. def Mapdisplay(center, dicc, Tiles=&quot;OpensTreetMap&quot;,zoom_start=10): &#39;&#39;&#39; :param center: Center of the map (Latitude and Longitude). :param dicc: Earth Engine Geometries or Tiles dictionary :param Tiles: Mapbox Bright,Mapbox Control Room,Stamen Terrain,Stamen Toner,stamenwatercolor,cartodbpositron. :zoom_start: Initial zoom level for the map. :return: A folium.Map object. &#39;&#39;&#39; mapViz = folium.Map(location=center,tiles=Tiles, zoom_start=zoom_start) for k,v in dicc.items(): if ee.image.Image in [type(x) for x in v.values()]: folium.TileLayer( tiles = v[&quot;tile_fetcher&quot;].url_format, attr = &#39;Google Earth Engine&#39;, overlay =True, name = k ).add_to(mapViz) else: folium.GeoJson( data = v, name = k ).add_to(mapViz) mapViz.add_child(folium.LayerControl()) return mapViz . # Query GEE for Landsat l8_image = ee.ImageCollection(&#39;LANDSAT/LC08/C01/T1_SR&#39;) .filterDate(&#39;2020-01-01&#39;, &#39;2020-08-31&#39;) .median() # Time to make a map l8_vis_params = { &#39;bands&#39;: [&#39;B4&#39;, &#39;B3&#39;, &#39;B2&#39;], &#39;min&#39;: 0, &#39;max&#39;: 3000, } Mapdisplay(center=[center.y, center.x], dicc={&#39;L8&#39;:l8_image.getMapId(l8_vis_params)}, zoom_start=12) . Make this Notebook Trusted to load map: File -&gt; Trust Notebook&lt;iframe src=&quot;about:blank&quot; style=&quot;position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;&quot; data-html=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 onload=&quot;this.contentDocument.open();this.contentDocument.write(atob(this.getAttribute(&#39;data-html&#39;)));this.contentDocument.close();&quot; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt;&lt;/iframe&gt; Exporting . At some point we often want to export data from Earth Engine for other uses. One might ask, can&#39;t we just do the whole analysis in Earth Engine? In some cases yes that&#39;s possible, but in other cases you need access to algorithms, data, or map making tools that just aren&#39;t possible in Earth Engine. . # But exporting the whole world to work with would be a pain, lets subset to our aoi band_sel = (&#39;B2&#39;, &#39;B3&#39;, &#39;B4&#39;, &#39;B5&#39;) l8_image = ee.ImageCollection(&#39;LANDSAT/LC08/C01/T1_SR&#39;) .filterDate(&#39;2020-01-01&#39;, &#39;2020-08-31&#39;) .select(band_sel) .filterBounds(gee_aoi) .median() l8_image.getInfo() . {&#39;bands&#39;: [{&#39;crs&#39;: &#39;EPSG:4326&#39;, &#39;crs_transform&#39;: [1, 0, 0, 0, 1, 0], &#39;data_type&#39;: {&#39;max&#39;: 32767, &#39;min&#39;: -32768, &#39;precision&#39;: &#39;double&#39;, &#39;type&#39;: &#39;PixelType&#39;}, &#39;id&#39;: &#39;B2&#39;}, {&#39;crs&#39;: &#39;EPSG:4326&#39;, &#39;crs_transform&#39;: [1, 0, 0, 0, 1, 0], &#39;data_type&#39;: {&#39;max&#39;: 32767, &#39;min&#39;: -32768, &#39;precision&#39;: &#39;double&#39;, &#39;type&#39;: &#39;PixelType&#39;}, &#39;id&#39;: &#39;B3&#39;}, {&#39;crs&#39;: &#39;EPSG:4326&#39;, &#39;crs_transform&#39;: [1, 0, 0, 0, 1, 0], &#39;data_type&#39;: {&#39;max&#39;: 32767, &#39;min&#39;: -32768, &#39;precision&#39;: &#39;double&#39;, &#39;type&#39;: &#39;PixelType&#39;}, &#39;id&#39;: &#39;B4&#39;}, {&#39;crs&#39;: &#39;EPSG:4326&#39;, &#39;crs_transform&#39;: [1, 0, 0, 0, 1, 0], &#39;data_type&#39;: {&#39;max&#39;: 32767, &#39;min&#39;: -32768, &#39;precision&#39;: &#39;double&#39;, &#39;type&#39;: &#39;PixelType&#39;}, &#39;id&#39;: &#39;B5&#39;}], &#39;type&#39;: &#39;Image&#39;} . #Only run this when necessary task.start() . Python &amp; Map Data . Now that we&#39;ve exported data from Google Earth Engine to Google Drive, we can practice loading data and working with spatial data in python. . %pip install rasterio #%pip install geopandas # remember we already did this earlier . Collecting rasterio Downloading https://files.pythonhosted.org/packages/34/2b/c8de31dc2767ef1cdcec980b3fe041776262bcdc859417babaeaad42cf3f/rasterio-1.1.6-cp36-cp36m-manylinux1_x86_64.whl (18.3MB) |████████████████████████████████| 18.3MB 228kB/s Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from rasterio) (20.2.0) Collecting affine Downloading https://files.pythonhosted.org/packages/ac/a6/1a39a1ede71210e3ddaf623982b06ecfc5c5c03741ae659073159184cd3e/affine-2.3.0-py2.py3-none-any.whl Requirement already satisfied: cligj&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from rasterio) (0.5.0) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rasterio) (1.18.5) Requirement already satisfied: click&lt;8,&gt;=4.0 in /usr/local/lib/python3.6/dist-packages (from rasterio) (7.1.2) Requirement already satisfied: click-plugins in /usr/local/lib/python3.6/dist-packages (from rasterio) (1.1.1) Collecting snuggs&gt;=1.4.1 Downloading https://files.pythonhosted.org/packages/cc/0e/d27d6e806d6c0d1a2cfdc5d1f088e42339a0a54a09c3343f7f81ec8947ea/snuggs-1.4.7-py3-none-any.whl Requirement already satisfied: pyparsing&gt;=2.1.6 in /usr/local/lib/python3.6/dist-packages (from snuggs&gt;=1.4.1-&gt;rasterio) (2.4.7) Installing collected packages: affine, snuggs, rasterio Successfully installed affine-2.3.0 rasterio-1.1.6 snuggs-1.4.7 . import rasterio as rio import geopandas %matplotlib inline . # Loading a raster and making a map raster_path = os.path.join(output_mount_folder, output_drive_folder, raster_name) l8_raster_path = &quot;.&quot;.join([raster_path, &quot;tif&quot;]) # Check some attributes with rio.open(l8_raster_path, &#39;r&#39;) as l8_raster: print(l8_raster.bounds) print(l8_raster.meta) . BoundingBox(left=85.26700879028404, bottom=27.665505636557317, right=85.38235247276498, top=27.747431990469018) {&#39;driver&#39;: &#39;GTiff&#39;, &#39;dtype&#39;: &#39;float64&#39;, &#39;nodata&#39;: None, &#39;width&#39;: 428, &#39;height&#39;: 304, &#39;count&#39;: 4, &#39;crs&#39;: CRS.from_epsg(4326), &#39;transform&#39;: Affine(0.00026949458523585647, 0.0, 85.26700879028404, 0.0, -0.00026949458523585647, 27.747431990469018)} . import matplotlib.pyplot as plt from rasterio.plot import show %matplotlib inline with rio.open(l8_raster_path, &#39;r&#39;) as l8_raster: fig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, nrows=1, figsize=(10, 4), sharey=True) # Plot Red, Green and Blue (rgb) show((l8_raster, 1), cmap=&#39;Blues&#39;, ax=ax1) show((l8_raster, 3), cmap=&#39;Reds&#39;, ax=ax3) show((l8_raster, 2), cmap=&#39;Greens&#39;, ax=ax2) show((l8_raster, 4), cmap=&#39;magma&#39;, ax=ax4) # Add titles ax1.set_title(&quot;Blue&quot;) ax2.set_title(&quot;Green&quot;) ax3.set_title(&quot;Red&quot;) ax4.set_title(&quot;NIR&quot;) . from rasterio.plot import reshape_as_image import numpy # Function to normalize the grid values def normalize(array): &quot;&quot;&quot;Normalizes numpy arrays into scale 0.0 - 1.0&quot;&quot;&quot; array_min, array_max = array.min(), array.max() return ((array - array_min)/(array_max - array_min)) # Open and read the raster data with rio.open(l8_raster_path, &#39;r&#39;) as l8_raster: l8_data = l8_raster.read() # To plot as RGB we have to normalize the data l8_image = numpy.empty(l8_data.shape, dtype=numpy.float) for band in range(l8_data.shape[0]): l8_image[band] = normalize(l8_data[band]) fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(10, 4), sharey=True) show(l8_image[[2,1,0],:,:], transform=l8_raster.transform, adjust=&#39;linear&#39;, ax=ax1) # RGB show(l8_image[[3,2,1],:,:], transform=l8_raster.transform, adjust=&#39;linear&#39;, ax=ax2) # False Color IR . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f79b0c9c630&gt; .",
            "url": "https://developmentseed.github.io/sat-ml-training/GettingStarted",
            "relUrl": "/GettingStarted",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Crop type mapping with deep learning",
            "content": "The AOI we will be working with is located in South Africa. We will use data from the 2019 Zindi Farm Pin Crop Detection Challenge and an abridged pipeline from Sinergise&#39;s eo-flow. The architecture we will use is the TensorFlow based TFCN. . Install eo-flow . !pip install git+https://github.com/sentinel-hub/eo-flow . Download data . Make an account on Zindi and proceed to https://zindi.africa/competitions/farm-pin-crop-detection-challenge/data to download the training and testing shapefiles: train.zip and test.zip . Import required libraries . import os # Jupyter notebook related %reload_ext autoreload %autoreload 2 %matplotlib inline %pylab inline # Basics of Python data handling and visualization import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec from mpl_toolkits.axes_grid1 import make_axes_locatable import pandas as pd from shapely.geometry import Polygon # Basics of GIS import geopandas as gpd # The core of this example from eolearn.core import EOTask, EOPatch, LinearWorkflow, FeatureType, OverwritePermission, LoadFromDisk, SaveToDisk from eolearn.io import S2L1CWCSInput, ExportToTiff from eolearn.mask import AddCloudMaskTask, get_s2_pixel_cloud_detector, AddValidDataMaskTask from eolearn.geometry import VectorToRaster, PointSamplingTask, ErosionTask from eolearn.features import LinearInterpolation, SimpleFilterTask from sentinelhub import BBoxSplitter, BBox, CRS, CustomUrlParam # Machine learning import lightgbm as lgb from sklearn.externals import joblib from sklearn import metrics from sklearn import preprocessing # Misc import pickle import sys import os import datetime import itertools from tqdm import tqdm_notebook as tqdm import enum . Preprocess data . 1) Generate convex hull geometries enveloping the training and testing shapefiles, to serve as AOI geometries used when generating EOPatches with Sentinel 2 imagery. . 2) Split the AOI into smaller tiles . 3) Fill EOPatches with data, to include: . L1C list of select bands [B02, B03, B04, B08, B11, B12], corresponding to [B, G, R, NIR, SWIR1, SWIR2] wavelengths. | Cloud probability map and cloud mask from SentinelHub | NDVI, NDWI, euclidean NORM information, which we will calculate | A mask of pixel validity, derived from the acquired Senitnel data and cloud coverage information. We define a valid pixel if its metadata: IS_DATA == True, CLOUD_MASK == 0 (1 indicates that pixel was identified to be occluded by cloud) | . Import eo-flow modules . import tensorflow as tf import json from eoflow.models import TFCNModel from eoflow.input.eopatch import eopatch_dataset from eoflow.input.operations import augment_data, cache_dataset, extract_subpatches from eoflow.utils import create_dirs . Convert EOPatch data to tfrecords . tfrecord is TensorFlow&#39;s dataset format optimized for ML workflows . Augment the data . Horizontal and vertical flips . Configure the model hyperparameters . To include: . learning rate | number of classes | loss | metrics | number of iterations and/or epochs (training cycles) | . Train model . Predict with the trained model . Evaluate the trained model . We will calculate Intersection over Union as a measure for the quality of our model. . Visualize predicted crop type maps .",
            "url": "https://developmentseed.github.io/sat-ml-training/croptype_deeplearning",
            "relUrl": "/croptype_deeplearning",
            "date": " • Jul 28, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "LightGBM  for Crop Type and Land Classification",
            "content": "This notebook teaches you to read satellite imagery (Sentinal-2) from AWS public S3 Buckets through AWS Public Data Program for crop type mapping. . #install python packages to run this notebook !pip install -q rasterio rasterstats geopandas treeinterpreter lightgbm . import geopandas as gpd import matplotlib.pyplot as plt import numpy as np import lightgbm as lgb import rasterio import rasterstats from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split from os import path as op import pickle . # Mount drive to google colab # from google.colab import drive # drive.mount(&#39;/content/drive&#39;, force_remount=True) # root_dir = &quot;/content/drive/My Drive&quot; . Random Forest Model for Crop Type and Land Classification . Using data created by SERVIR East Africa, RCMRD, and FEWSNET, we demonstrate how to train a LightGBM classifier over Trans Nzoia county, Kenya. . # read in training data training_vectors = gpd.read_file(op.join(root_dir, &#39;training_data.geojson&#39;)) training_vectors.head() . name description geometry . 0 Shadow | None | MULTIPOLYGON (((34.83383 1.18204, 34.83577 1.1... | . 1 Forestland | None | MULTIPOLYGON (((35.30961 1.01328, 35.30964 1.0... | . 2 Maize | early reproductive | MULTIPOLYGON (((34.90904 1.09515, 34.90907 1.0... | . 3 Sugarcane | no change..maize farm on the right and far lef... | MULTIPOLYGON (((34.90750 1.08934, 34.90753 1.0... | . 4 Maize | reproductive good crop | MULTIPOLYGON (((34.87144 0.82953, 34.87147 0.8... | . # find all unique values of training data names to use as classes classes = np.unique(training_vectors.name) # classes = np.array(sorted(training_vectors.name.unique())) classes . array([&#39;Built&#39;, &#39;Cloud&#39;, &#39;Fallow&#39;, &#39;Forestland&#39;, &#39;Grassland&#39;, &#39;Maize&#39;, &#39;Shadow&#39;, &#39;Sugarcane&#39;, &#39;Sunflower&#39;, &#39;Waterbody&#39;], dtype=object) . # create a dictionary to convert class names into integers for modeling class_dict = dict(zip(classes, range(len(classes)))) class_dict . {&#39;Built&#39;: 0, &#39;Cloud&#39;: 1, &#39;Fallow&#39;: 2, &#39;Forestland&#39;: 3, &#39;Grassland&#39;: 4, &#39;Maize&#39;: 5, &#39;Shadow&#39;: 6, &#39;Sugarcane&#39;: 7, &#39;Sunflower&#39;: 8, &#39;Waterbody&#39;: 9} . %%time # this larger cell reads data from a raster file for each training vector import rasterio from rasterio.features import rasterize from rasterstats.io import bounds_window # raster information raster_file = op.join(root_dir, &#39;Trans_nzoia_2019_05-02.tif&#39;) bands = 6 # a custom function for getting each value from the raster def all_values(x): return x # this larger cell reads data from a raster file for each training vector X_raw = [] y_raw = [] with rasterio.open(raster_file, &#39;r&#39;) as src: for (label, geom) in zip(training_vectors.name, training_vectors.geometry): # read the raster data matching the geometry bounds window = bounds_window(geom.bounds, src.transform) # store our window information window_affine = src.window_transform(window) fsrc = src.read(window=window) # rasterize the (non-buffered) geometry into the larger shape and affine mask = rasterize( [(geom, 1)], out_shape=fsrc.shape[1:], transform=window_affine, fill=0, dtype=&#39;uint8&#39;, all_touched=True ).astype(bool) # for each label pixel (places where the mask is true)... label_pixels = np.argwhere(mask) for (row, col) in label_pixels: # add a pixel of data to X data = fsrc[:,row,col] one_x = np.nan_to_num(data, nan=1e-3) X_raw.append(one_x) # add the label to y y_raw.append(class_dict[label]) . CPU times: user 13.2 s, sys: 457 ms, total: 13.6 s Wall time: 20.4 s . # convert the training data lists into the appropriate shape and format for scikit-learn X = np.array(X_raw) y = np.array(y_raw) (X.shape, y.shape) . ((160461, 6), (160461,)) . # (optional) add extra band indices # helper function for calculating ND*I indices (bands in the final dimension) def band_index(arr, a, b): return np.expand_dims((arr[..., a] - arr[..., b]) / (arr[..., a] + arr[..., b]), axis=1) ndvi = band_index(X, 3, 2) ndwi = band_index(X, 1, 3) X = np.concatenate([X, ndvi, ndwi], axis=1) X.shape . (160461, 8) . # split the data into test and train sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) . # calculate class weights to allow for training on inbalanced training samples labels, counts = np.unique(y_train, return_counts=True) class_weight_dict = dict(zip(labels, 1 / counts)) class_weight_dict . {0: 0.00046882325363338024, 1: 0.001597444089456869, 2: 0.0004928536224741252, 3: 1.970093973482535e-05, 4: 0.000819000819000819, 5: 1.5704257424187697e-05, 6: 0.0002473410833539451, 7: 0.0002824858757062147, 8: 0.05263157894736842, 9: 0.003115264797507788} . %%time # initialize a lightgbm lgbm = lgb.LGBMClassifier( objective=&#39;multiclass&#39;, class_weight = class_weight_dict, num_class = len(class_dict), metric = &#39;multi_logloss&#39;) . CPU times: user 79 µs, sys: 5 µs, total: 84 µs Wall time: 90.4 µs . # fit the model to the data (training) lgbm.fit(X_train, y_train) . LGBMClassifier(boosting_type=&#39;gbdt&#39;, class_weight={0: 0.00046882325363338024, 1: 0.001597444089456869, 2: 0.0004928536224741252, 3: 1.970093973482535e-05, 4: 0.000819000819000819, 5: 1.5704257424187697e-05, 6: 0.0002473410833539451, 7: 0.0002824858757062147, 8: 0.05263157894736842, 9: 0.003115264797507788}, colsample_bytree=1.0, importance_type=&#39;split&#39;, learning_rate=0.1, max_depth=-1, metric=&#39;multi_logloss&#39;, min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_class=10, num_leaves=31, objective=&#39;multiclass&#39;, random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0, subsample_for_bin=200000, subsample_freq=0) . # predict on X_test to evaluate the model preds = lgbm.predict(X_test) cm = confusion_matrix(y_test, preds, labels=labels) . model_name = &#39;light_gbm.sav&#39; pickle.dump(lgbm, open(op.join(root_dir, model_name), &#39;wb&#39;)) . # plot the confusion matrix %matplotlib inline cm = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis] fig, ax = plt.subplots(figsize=(10, 10)) im = ax.imshow(cm, interpolation=&#39;nearest&#39;, cmap=plt.cm.Blues) ax.figure.colorbar(im, ax=ax) # We want to show all ticks... ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), # ... and label them with the respective list entries xticklabels=classes, yticklabels=classes, title=&#39;Normalized Confusion Matrix&#39;, ylabel=&#39;True label&#39;, xlabel=&#39;Predicted label&#39;) # Rotate the tick labels and set their alignment. plt.setp(ax.get_xticklabels(), rotation=45, ha=&quot;right&quot;, rotation_mode=&quot;anchor&quot;) # Loop over data dimensions and create text annotations. fmt = &#39;.2f&#39; thresh = cm.max() / 2. for i in range(cm.shape[0]): for j in range(cm.shape[1]): ax.text(j, i, format(cm[i, j], fmt), ha=&quot;center&quot;, va=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) fig.tight_layout() . Generate predictions over the full image . # if want to use the pretrained model for new imagery # helper function for calculating ND*I indices (bands in the final dimension) # match the pretrained model weight with the saved model above model_name = &#39;light_gbm.sav&#39; def band_index(arr, a, b): return np.expand_dims((arr[..., a] - arr[..., b]) / (arr[..., a] + arr[..., b]), axis=1) lgbm = pickle.load(open(op.join(root_dir, model_name), &#39;rb&#39;)) . lgbm . LGBMClassifier(boosting_type=&#39;gbdt&#39;, class_weight={0: 0.00046882325363338024, 1: 0.001597444089456869, 2: 0.0004928536224741252, 3: 1.970093973482535e-05, 4: 0.000819000819000819, 5: 1.5704257424187697e-05, 6: 0.0002473410833539451, 7: 0.0002824858757062147, 8: 0.05263157894736842, 9: 0.003115264797507788}, colsample_bytree=1.0, importance_type=&#39;split&#39;, learning_rate=0.1, max_depth=-1, metric=&#39;multi_logloss&#39;, min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_class=10, num_leaves=31, objective=&#39;multiclass&#39;, random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0, subsample_for_bin=200000, subsample_freq=0) . # src.close() # dst.close() . # if want to use the pretrained model for new imagery # The pretrained model is called &quot;random_forest.sav&quot; # helper function for calculating ND*I indices (bands in the final dimension) # open connections to our input and output images # new_image = op.join(root_dir, &#39;Trans_nzoia_2019_10-04.tif&#39;) new_image = raster_file output_image = op.join(root_dir, &quot;lgbm_classification.tif&quot;) src = rasterio.open(new_image, &#39;r&#39;) profile = src.profile profile.update( dtype=rasterio.uint8, count=1, ) dst = rasterio.open(output_image, &#39;w&#39;, **profile) # perform prediction on each small image patch to minimize required memory patch_size = 500 for i in range((src.shape[0] // patch_size) + 1): for j in range((src.shape[1] // patch_size) + 1): # define the pixels to read (and write) window = rasterio.windows.Window( j * patch_size, i * patch_size, # don&#39;t read past the image bounds min(patch_size, src.shape[1] - j * patch_size), min(patch_size, src.shape[0] - i * patch_size) ) data = src.read(window=window) # read the image into the proper format, adding indices if necessary img_swp = np.moveaxis(data, 0, 2) img_flat = img_swp.reshape(-1, img_swp.shape[-1]) img_ndvi = band_index(img_flat, 3, 2) img_ndwi = band_index(img_flat, 1, 3) img_w_ind = np.concatenate([img_flat, img_ndvi, img_ndwi], axis=1) # remove no data values, store the indices for later use # a later cell makes the assumption that all bands have identical no-data value arrangements m = np.ma.masked_invalid(img_w_ind) to_predict = img_w_ind[~m.mask].reshape(-1, img_w_ind.shape[-1]) # predict if not len(to_predict): continue img_preds = lgbm.predict(to_predict) # add the prediction back to the valid pixels (using only the first band of the mask to decide on validity) # resize to the original image dimensions output = np.zeros(img_flat.shape[0]) output[~m.mask[:,0]] = img_preds.flatten() output = output.reshape(*img_swp.shape[:-1]) # create our final mask mask = (~m.mask[:,0]).reshape(*img_swp.shape[:-1]) # write to the final file dst.write(output.astype(rasterio.uint8), 1, window=window) dst.write_mask(mask, window=window) # write to the final file dst.write(output.astype(rasterio.uint8), 1, window=window) dst.write_mask(mask, window=window) src.close() dst.close() .",
            "url": "https://developmentseed.github.io/sat-ml-training/LightGBM_cropmapping",
            "relUrl": "/LightGBM_cropmapping",
            "date": " • Feb 24, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Crop type mapping with deep learning",
            "content": "In this tutorial we will learn how to segment images. Segmentation is the process of generating pixel-wise segmentations giving the class of the object visible at each pixel. For example, we could be identifying the location and boundaries of people within an image or identifying cell nuclei from an image. Formally, image segmentation refers to the process of partitioning an image into a set of pixels that we desire to identify (our target) and the background. . Specifically, in this tutorial we will be using the Farm Pin Crop Detection Challenge. . This challenge provides ground truth crop type labels with multiple Sentinel 2 scenes captured at different timesteps between January and August of 2017. The area of interest lies along a section of the Orange River in South Africa. Our task will be to predict the crop types in an image on a pixel-wise basis. . Specific concepts that will be covered: . In the process, we will build practical experience and develop intuition around the following concepts: . Functional API - we will be implementing UNet, a convolutional network model classically used for biomedical image segmentation with the Functional API. This model has layers that require multiple input/outputs. This requires the use of the functional API | Check out the original paper, U-Net: Convolutional Networks for Biomedical Image Segmentation by Olaf Ronneberger! | . | Loss Functions and Metrics - We&#39;ll implement the Sparse Categorical focal loss function (https://focal-loss.readthedocs.io/en/latest/) and accuracy. We&#39;ll also implement mean intersection over union during evaluation and and generate confusion matrices during evaluation to judge how well the model performs. | Saving and loading keras models - We&#39;ll save our best model to file. When we want to perform inference/evaluate our model in the future, we can load the model file. | . We will follow the general workflow: . Load ZINDI datasets from Google Drive | Compute spectral indices useful for crop type mapping | Visualize data/perform some exploratory data analysis | Set up data pipeline and preprocessing | Build model | Train model | Test model | Evaluate model | Audience: This post is geared towards intermediate users who are comfortable with basic machine learning concepts. . Time Estimated: 120 min . from google.colab import drive drive.mount(&#39;/content/gdrive&#39;) . root_dir = &#39;gdrive/My Drive/croptype/&#39; . %cd $root_dir . !pip install rasterio !pip install geopandas !pip install git+https://github.com/tensorflow/examples.git !pip install -U tfds-nightly !pip install focal-loss . import os import glob import functools from zipfile import ZipFile import fnmatch from itertools import product import numpy as np import matplotlib.pyplot as plt import matplotlib as mpl mpl.rcParams[&#39;axes.grid&#39;] = False mpl.rcParams[&#39;figure.figsize&#39;] = (12,12) from sklearn.model_selection import train_test_split import matplotlib.image as mpimg import pandas as pd from PIL import Image import rasterio from rasterio import features from rasterio import mask from rasterio import windows import geopandas as gpd import tensorflow as tf from tensorflow.python.keras import layers from tensorflow.python.keras import losses from tensorflow.python.keras import models from tensorflow.python.keras import backend as K import tensorflow_addons as tfa from tensorflow_examples.models.pix2pix import pix2pix import tensorflow_datasets as tfds tfds.disable_progress_bar() from IPython.display import clear_output import matplotlib.pyplot as plt from focal_loss import SparseCategoricalFocalLoss from sklearn.metrics import confusion_matrix . Get all the data . We&#39;ll download the data from ZINDI . The data is available on our shared google drive. Or you can register an account on https://zindi.africa/ and download other or more Sentinel 2 timestamps from the competition. . The training labels: . train.zip . Four Sentinel 2 collects from 2017 to start (March 22, May 31, June 20 and August 4). See the document &quot;OrangeRiver_Climate.docx&quot; provided in the ZINDI competition data for information on the local climate and growing season. . 2017-03-22.zip 2017-08-04.zip 2017-05-31.zip 2017-06-20.zip . root_dir = &#39;./&#39; sentinel_timestamps = [&#39;2017-03-22&#39;, &#39;2017-05-31&#39;, &#39;2017-06-20&#39;, &#39;2017-08-04&#39;] sentinel_timestamp = sentinel_timestamps[1] target_crs = &#39;epsg:32734&#39; . # Unzip all of the ZINDI competition data for z in glob.glob(&#39;./*.zip&#39;): filename_split = os.path.splitext(z) filename_zero, fileext = filename_split basename = os.path.basename(filename_zero) with ZipFile(z, &#39;r&#39;) as zf: zf.extractall(basename) . Timestamp processing start . Get the band images. We only need Sentinel-2&#39;s Band 2, 3, 4, and 8 (blue, green, red, NIR) to compute the spectral indices of use. . def sentinel_read(sentinel_timestamp): sentinel_dir = os.path.join(root_dir,sentinel_timestamp) bands = glob.glob(sentinel_dir+&#39;/**/*.jp2&#39;,recursive=True) # Read band metadata and arrays # metadata src_2 = rasterio.open(fnmatch.filter(bands, &#39;*B02.jp2&#39;)[0]) src_3 = rasterio.open(fnmatch.filter(bands, &#39;*B03.jp2&#39;)[0]) src_4 = rasterio.open(fnmatch.filter(bands, &#39;*B04.jp2&#39;)[0]) src_8 = rasterio.open(fnmatch.filter(bands, &#39;*B08.jp2&#39;)[0]) # array arr_2 = src_2.read() arr_3 = src_3.read() arr_4 = src_4.read() arr_8 = src_8.read() return sentinel_dir, arr_2, arr_3, arr_4, arr_8, src_8 . Now let&#39;s calculate the spectral indices . NDVI: Normalized Difference Vegetation Index SAVI: Soil Adjusted Vegetation Index WDRVI: Wide Dynamic Range Vegetation Index . def indexnormstack(blue, green, red, nir): def NDIcalc(nir, red): ndi = (nir - red) / (nir + red + 1e-5) return ndi def GARIcalc(blue, green, red, nir): gamma = 1.7 gari = (nir - (green - (gamma * (blue - red)))) / (nir + (green - (gamma * (blue - red)))) return gari def OSAVIcalc(red, nir): osavi = (nir - red) / (nir + red + 0.16) return osavi def WRDVIcalc(red,nir): a = 0.2 wdrvi = (a * nir - red) / (a * nir - red) return wdrvi def SAVIcalc(red, nir): savi = 1.5 * (nir - red) / (nir + red + 0.5) return savi def EVIcalc(blue,red,nir): evi = (nir - red) / (nir + 6 * red - 7.5 * blue - 1) return evi def norm(arr): arr_norm = (255*(arr - np.min(arr))/np.ptp(arr)) return arr_norm ndvi = NDIcalc(nir,red) #ndvi_norm = norm(ndvi) savi = SAVIcalc(red,nir) #savi_norm = norm(savi) wdrvi = WRDVIcalc(red,nir) #wdrvi_norm = norm(wdrvi) ndvi = ndvi.transpose(1,2,0) savi = savi.transpose(1,2,0) wdrvi = wdrvi.transpose(1,2,0) index_stack = np.dstack((ndvi, savi, wdrvi)) return index_stack . Read label shapefile into geopandas dataframe, check for invalid geometries and set to local CRS. Then, rasterize the labeled polygons using the metadata from one of the grayscale band images. . def label(geo, src_8): geo = gpd.read_file(geo) geo = geo.loc[geo.is_valid] geo = geo.to_crs(crs={&#39;init&#39;: target_crs}) geo[&#39;Crop_Id_Ne_int&#39;] = geo.Crop_Id_Ne.astype(int) shapes = ((geom,value) for geom, value in zip(geo.geometry, geo.Crop_Id_Ne_int)) src_8_prf = src_8.profile labels = features.rasterize(shapes=shapes, out_shape=(src_8_prf[&#39;height&#39;], src_8_prf[&#39;width&#39;]), fill=0, all_touched=True, transform=src_8_prf[&#39;transform&#39;], dtype=src_8_prf[&#39;dtype&#39;]) print(&quot;Check values in labeled image: &quot;, np.unique(labels)) return labels . def save_images(sentinel_dir, index_stack, labels, src_8): index_stack = (index_stack * 255).astype(np.uint8) index_stack_t = index_stack.transpose(2,0,1) labels = labels.astype(np.uint8) index_stack_out=rasterio.open(sentinel_dir+&#39;/index_stack.tiff&#39;, &#39;w&#39;, driver=&#39;Gtiff&#39;, width=src_8.width, height=src_8.height, count=3, crs=src_8.crs, transform=src_8.transform, dtype=&#39;uint8&#39;) index_stack_out.write(index_stack_t) index_stack_out.close() labels_out=rasterio.open(sentinel_dir+&#39;/labels.tiff&#39;, &#39;w&#39;, driver=&#39;Gtiff&#39;, width=src_8.width, height=src_8.height, count=1, crs=src_8.crs, transform=src_8.transform, dtype=&#39;uint8&#39;) labels_out.write(labels, 1) labels_out.close() . Now let&#39;s divide the Sentinel 2 index stack and labeled image into 224x224 pixel tiles . def tile(index_stack, labels): tiles_dir = root_dir+&#39;tiled/&#39; img_dir = root_dir+&#39;tiled/images/&#39; label_dir = root_dir+&#39;tiled/labels/&#39; dirs = [tiles_dir, img_dir, label_dir] for d in dirs: if not os.path.exists(d): os.makedirs(d) height,width = 224, 224 def get_tiles(ds, width=224, height=224): nols, nrows = ds.meta[&#39;width&#39;], ds.meta[&#39;height&#39;] offsets = product(range(0, nols, width), range(0, nrows, height)) big_window = windows.Window(col_off=0, row_off=0, width=nols, height=nrows) for col_off, row_off in offsets: window =windows.Window(col_off=col_off, row_off=row_off, width=width, height=height).intersection(big_window) transform = windows.transform(window, ds.transform) yield window, transform tile_width, tile_height = 224, 224 def crop(inpath, outpath, c): image = rasterio.open(inpath) meta = image.meta.copy() meta[&#39;count&#39;] = int(c) meta[&#39;driver&#39;]=&#39;PNG&#39; i = 0 for window, transform in get_tiles(image): meta[&#39;transform&#39;] = transform meta[&#39;width&#39;], meta[&#39;height&#39;] = window.width, window.height outfile = outpath+&quot;tile_%s_%s.png&quot; % (sentinel_timestamp, str(i)) with rasterio.open(outfile, &#39;w&#39;, **meta) as outds: outds.write(image.read(window=window)) i = i+1 def process_tiles(index_flag): if index_flag==True: inpath = sentinel_dir+&#39;/index_stack.tiff&#39; outpath=img_dir crop(inpath, outpath, 3) else: inpath = sentinel_dir+&#39;/labels.tiff&#39; outpath=label_dir crop(inpath, outpath, 1) process_tiles(index_flag=True) # tile index stack process_tiles(index_flag=False) # tile labels return tiles_dir, img_dir, label_dir . Run the image processing workflow . def main(timestamp): sentinel_dir, arr_2, arr_3, arr_4, arr_8, src_8 = sentinel_read(timestamp) # Calculate indices and combine the indices into one single 3 channel image index_stack = indexnormstack(arr_2, arr_3, arr_4, arr_8) # Rasterize labels labels = label(root_dir+&#39;train/train/train.shp&#39;, src_8) # Save index stack and labels to geotiff index_stack_file, labels_file = save_images(sentinel_dir, index_stack, labels, src_8) # Tile images into 224x224 tiles_dir, img_dir, label_dir = tile(index_stack, labels) return timestamp, tiles_dir, img_dir, label_dir for timestamp in sentinel_timestamps: timestamp, tiles_dir, img_dir, label_dir = main(timestamp) . . Read into tensorflow datasets . Now we will compile the spectral index image and label tiles into training, validation, and test datasets for use with TensorFlow. . train_imgs = glob.glob(img_dir+&quot;/*.png&quot;) train_list = [] for img in train_imgs: filename_split = os.path.splitext(img) filename_zero, fileext = filename_split basename = os.path.basename(filename_zero) train_list.append(basename) x_train_filenames = [] y_train_filenames = [] for img_id in train_list: x_train_filenames.append(os.path.join(img_dir, &quot;{}.png&quot;.format(img_id))) y_train_filenames.append(os.path.join(label_dir, &quot;{}.png&quot;.format(img_id))) print(len(train_list)) . Let&#39;s check for the proportion of background tiles. . background_list = [] for i in train_list: img = np.array(Image.open(os.path.join(label_dir, &quot;{}.png&quot;.format(i)))) if img.max()==0: background_list.append(i) print(len(background_list)) . 9620 . We will keep only 10% of the total. Too many background tiles can cause a form of class imbalance. . background_removal = len(background_list) * 0.9 train_list_clean = [y for y in train_list if y not in background_list[0:int(background_removal)]] x_train_filenames = [] y_train_filenames = [] for img_id in train_list_clean: x_train_filenames.append(os.path.join(img_dir, &quot;{}.png&quot;.format(img_id))) y_train_filenames.append(os.path.join(label_dir, &quot;{}.png&quot;.format(img_id))) print(len(train_list_clean)) . 1342 . Split index tiles and label tiles into train and test sets: 90% and 10%, respectively. . x_train_filenames, x_val_filenames, y_train_filenames, y_val_filenames = train_test_split(x_train_filenames, y_train_filenames, test_size=0.1, random_state=42) num_train_examples = len(x_train_filenames) num_val_examples = len(x_val_filenames) print(&quot;Number of training examples: {}&quot;.format(num_train_examples)) print(&quot;Number of validation examples: {}&quot;.format(num_val_examples)) . Number of training examples: 1207 Number of validation examples: 135 . Visualize the data . display_num = 3 foreground_list_x = [] foreground_list_y = [] for x,y in zip(x_train_filenames, y_train_filenames): img = np.array(Image.open(y)) if img.max()&gt;0: foreground_list_x.append(x) foreground_list_y.append(y) num_foreground_examples = len(foreground_list_y) r_choices = np.random.choice(num_foreground_examples, display_num) plt.figure(figsize=(10, 15)) for i in range(0, display_num * 2, 2): img_num = r_choices[i // 2] x_pathname = foreground_list_x[img_num] y_pathname = foreground_list_y[img_num] plt.subplot(display_num, 2, i + 1) plt.imshow(mpimg.imread(x_pathname)) plt.title(&quot;Original Image&quot;) example_labels = Image.open(y_pathname) label_vals = np.unique(np.array(example_labels)) plt.subplot(display_num, 2, i + 2) plt.imshow(example_labels) plt.title(&quot;Masked Image&quot;) plt.suptitle(&quot;Examples of Images and their Masks&quot;) plt.show() . Read the tiles into tensors . img_shape = (224, 224, 3) batch_size = 2 . def _process_pathnames(fname, label_path): # We map this function onto each pathname pair img_str = tf.io.read_file(fname) img = tf.image.decode_png(img_str, channels=3) label_img_str = tf.io.read_file(label_path) # These are png images so they return as (num_frames, h, w, c) label_img = tf.image.decode_png(label_img_str, channels=1) # The label image should have any values between 0 and 9, indicating pixel wise # cropt type class or background (0). We take the first channel only. label_img = label_img[:, :, 0] label_img = tf.expand_dims(label_img, axis=-1) return img, label_img . def flip_img_h(horizontal_flip, tr_img, label_img): if horizontal_flip: flip_prob = tf.random.uniform([], 0.0, 1.0) tr_img, label_img = tf.cond(tf.less(flip_prob, 0.5), lambda: (tf.image.flip_left_right(tr_img), tf.image.flip_left_right(label_img)), lambda: (tr_img, label_img)) return tr_img, label_img . def flip_img_v(vertical_flip, tr_img, label_img): if vertical_flip: flip_prob = tf.random.uniform([], 0.0, 1.0) tr_img, label_img = tf.cond(tf.less(flip_prob, 0.5), lambda: (tf.image.flip_up_down(tr_img), tf.image.flip_up_down(label_img)), lambda: (tr_img, label_img)) return tr_img, label_img . def _augment(img, label_img, resize=None, # Resize the image to some size e.g. [256, 256] scale=1, # Scale image e.g. 1 / 255. horizontal_flip=False, vertical_flip=False): if resize is not None: # Resize both images label_img = tf.image.resize(label_img, resize) img = tf.image.resize(img, resize) img, label_img = flip_img_h(horizontal_flip, img, label_img) img, label_img = flip_img_v(vertical_flip, img, label_img) img = tf.cast(img, tf.float32) * scale #tf.to_float(img) * scale #print(&quot;tensor: &quot;, tf.unique(tf.keras.backend.print_tensor(label_img))) return img, label_img . def get_baseline_dataset(filenames, labels, preproc_fn=functools.partial(_augment), threads=5, batch_size=batch_size, shuffle=True): num_x = len(filenames) # Create a dataset from the filenames and labels dataset = tf.data.Dataset.from_tensor_slices((filenames, labels)) # Map our preprocessing function to every element in our dataset, taking # advantage of multithreading dataset = dataset.map(_process_pathnames, num_parallel_calls=threads) if preproc_fn.keywords is not None and &#39;resize&#39; not in preproc_fn.keywords: assert batch_size == 1, &quot;Batching images must be of the same size&quot; dataset = dataset.map(preproc_fn, num_parallel_calls=threads) if shuffle: dataset = dataset.shuffle(num_x) # It&#39;s necessary to repeat our data for all epochs dataset = dataset.repeat().batch(batch_size) return dataset . tr_cfg = { &#39;resize&#39;: [img_shape[0], img_shape[1]], &#39;scale&#39;: 1 / 255., &#39;horizontal_flip&#39;: True, &#39;vertical_flip&#39;: True, } tr_preprocessing_fn = functools.partial(_augment, **tr_cfg) . val_cfg = { &#39;resize&#39;: [img_shape[0], img_shape[1]], &#39;scale&#39;: 1 / 255., } val_preprocessing_fn = functools.partial(_augment, **val_cfg) . train_ds = get_baseline_dataset(x_train_filenames, y_train_filenames, preproc_fn=tr_preprocessing_fn, batch_size=batch_size) val_ds = get_baseline_dataset(x_val_filenames, y_val_filenames, preproc_fn=val_preprocessing_fn, batch_size=batch_size) . display_num = 1 r_choices = np.random.choice(num_foreground_examples, 1) for i in range(0, display_num * 2, 2): img_num = r_choices[i // 2] temp_ds = get_baseline_dataset(foreground_list_x[img_num:img_num+1], foreground_list_y[img_num:img_num+1], preproc_fn=tr_preprocessing_fn, batch_size=1, shuffle=False) # Let&#39;s examine some of these augmented images iterator = iter(temp_ds) next_element = iterator.get_next() batch_of_imgs, label = next_element # Running next element in our graph will produce a batch of images sample_image, sample_mask = batch_of_imgs[0], label[0,:,:,:] . def display(display_list): plt.figure(figsize=(15, 15)) title = [&#39;Input Image&#39;, &#39;True Mask&#39;, &#39;Predicted Mask&#39;] for i in range(len(display_list)): plt.subplot(1, len(display_list), i+1) plt.title(title[i]) plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i])) plt.axis(&#39;off&#39;) plt.show() . display([sample_image, sample_mask]) . ...same check for the validation images: . foreground_list_x = [] foreground_list_y = [] for x,y in zip(x_val_filenames, y_val_filenames): img = np.array(Image.open(y)) if img.max()&gt;0: foreground_list_x.append(x) foreground_list_y.append(y) num_foreground_examples = len(foreground_list_y) . display_num = 1 r_choices = np.random.choice(num_foreground_examples, 1) for i in range(0, display_num * 2, 2): img_num = r_choices[i // 2] temp_ds = get_baseline_dataset(foreground_list_x[img_num:img_num+1], foreground_list_y[img_num:img_num+1], preproc_fn=tr_preprocessing_fn, batch_size=1, shuffle=False) # Let&#39;s examine some of these augmented images iterator = iter(temp_ds) next_element = iterator.get_next() batch_of_imgs, label = next_element # Running next element in our graph will produce a batch of images sample_image, sample_mask = batch_of_imgs[0], label[0,:,:,:] . display([sample_image, sample_mask]) . Define the model . The model being used here is a modified U-Net. A U-Net consists of an encoder (downsampler) and decoder (upsampler). In-order to learn robust features, and reduce the number of trainable parameters, a pretrained model can be used as the encoder. Thus, the encoder for this task will be a pretrained MobileNetV2 model, whose intermediate outputs will be used, and the decoder will be the upsample block already implemented in TensorFlow Examples in the Pix2pix tutorial. . The reason to output ten channels is because there are ten possible labels for each pixel. Think of this as multi-classification where each pixel is being classified into ten classes. . OUTPUT_CHANNELS = 10 . As mentioned, the encoder will be a pretrained MobileNetV2 model which is prepared and ready to use in tf.keras.applications. The encoder consists of specific outputs from intermediate layers in the model. Note that the encoder will not be trained during the training process. . base_model = tf.keras.applications.MobileNetV2(input_shape=[224, 224, 3], include_top=False) # Use the activations of these layers layer_names = [ &#39;block_1_expand_relu&#39;, # 64x64 &#39;block_3_expand_relu&#39;, # 32x32 &#39;block_6_expand_relu&#39;, # 16x16 &#39;block_13_expand_relu&#39;, # 8x8 &#39;block_16_project&#39;, # 4x4 ] layers = [base_model.get_layer(name).output for name in layer_names] # Create the feature extraction model down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers) down_stack.trainable = False . Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5 9412608/9406464 [==============================] - 0s 0us/step . The decoder/upsampler is simply a series of upsample blocks implemented in TensorFlow examples. . up_stack = [ pix2pix.upsample(512, 3), # 4x4 -&gt; 8x8 pix2pix.upsample(256, 3), # 8x8 -&gt; 16x16 pix2pix.upsample(128, 3), # 16x16 -&gt; 32x32 pix2pix.upsample(64, 3), # 32x32 -&gt; 64x64 ] . def unet_model(output_channels): inputs = tf.keras.layers.Input(shape=[224,224,3]) x = inputs # Downsampling through the model skips = down_stack(x) x = skips[-1] skips = reversed(skips[:-1]) # Upsampling and establishing the skip connections for up, skip in zip(up_stack, skips): x = up(x) concat = tf.keras.layers.Concatenate() x = concat([x, skip]) # This is the last layer of the model last = tf.keras.layers.Conv2DTranspose( output_channels, 3, strides=2, padding=&#39;same&#39;) #64x64 -&gt; 224x224 x = last(x) return tf.keras.Model(inputs=inputs, outputs=x) . Train the model . Now, all that is left to do is to compile and train the model. The loss being used here is losses.SparseCategoricalCrossentropy(from_logits=True). The reason to use this loss function is because the network is trying to assign each pixel a label, just like multi-class prediction. In the true segmentation mask, each pixel has a value between 0-9. The network here is outputting ten channels. Essentially, each channel is trying to learn to predict a class, and losses.SparseCategoricalCrossentropy(from_logits=True) is the recommended loss for such a scenario. Using the output of the network, the label assigned to the pixel is the channel with the highest value. This is what the create_mask function is doing. . model = unet_model(OUTPUT_CHANNELS) . Notice there is a class imbalance problem in the dataset. For that reason, we will use a loss function called focal loss. It uses a parameter to weigh the losses contributed by each class to prevent bias towards the over-represented. . train_df = pd.read_csv(&#39;Farmpin_training.csv&#39;) inv_freq = np.array(1/(train_df.crop_id.value_counts()/len(train_df))) inv_freq = [0.,*inv_freq] class_weights = {0 : inv_freq[0], 1: inv_freq[1], 2: inv_freq[2], 3: inv_freq[3], 4: inv_freq[4], 5: inv_freq[5], 6: inv_freq[6], 7: inv_freq[7], 8: inv_freq[8], 9: inv_freq[9]} . class_weights . {0: 0.0, 1: 2.647932131495228, 2: 4.6585820895522385, 3: 8.823321554770319, 4: 9.352059925093632, 5: 16.98639455782313, 6: 17.58450704225352, 7: 26.28421052631579, 8: 32.42857142857143, 9: 356.7142857142857} . We will measure our model&#39;s performance during training by per-pixel accuracy. . model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001), loss=SparseCategoricalFocalLoss(gamma=2, from_logits=True), metrics=[&#39;accuracy&#39;]) . Have a quick look at the resulting model architecture: . model.summary() . Model: &#34;functional_3&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_2 (InputLayer) [(None, 224, 224, 3) 0 __________________________________________________________________________________________________ functional_1 (Functional) [(None, 112, 112, 96 1841984 input_2[0][0] __________________________________________________________________________________________________ sequential (Sequential) (None, 14, 14, 512) 1476608 functional_1[0][4] __________________________________________________________________________________________________ concatenate (Concatenate) (None, 14, 14, 1088) 0 sequential[0][0] functional_1[0][3] __________________________________________________________________________________________________ sequential_1 (Sequential) (None, 28, 28, 256) 2507776 concatenate[0][0] __________________________________________________________________________________________________ concatenate_1 (Concatenate) (None, 28, 28, 448) 0 sequential_1[0][0] functional_1[0][2] __________________________________________________________________________________________________ sequential_2 (Sequential) (None, 56, 56, 128) 516608 concatenate_1[0][0] __________________________________________________________________________________________________ concatenate_2 (Concatenate) (None, 56, 56, 272) 0 sequential_2[0][0] functional_1[0][1] __________________________________________________________________________________________________ sequential_3 (Sequential) (None, 112, 112, 64) 156928 concatenate_2[0][0] __________________________________________________________________________________________________ concatenate_3 (Concatenate) (None, 112, 112, 160 0 sequential_3[0][0] functional_1[0][0] __________________________________________________________________________________________________ conv2d_transpose_4 (Conv2DTrans (None, 224, 224, 10) 14410 concatenate_3[0][0] ================================================================================================== Total params: 6,514,314 Trainable params: 4,670,410 Non-trainable params: 1,843,904 __________________________________________________________________________________________________ . Let&#39;s try out the pre-trained model to see what it predicts before training. . def create_mask(pred_mask): pred_mask = tf.argmax(pred_mask, axis=-1) pred_mask = pred_mask[..., tf.newaxis] return pred_mask[0] . def show_predictions(dataset=None, num=1): if dataset: for image, mask in dataset.take(num): pred_mask = model.predict(image) display([image[0], mask[0], create_mask(pred_mask)]) else: mp = create_mask(model.predict(sample_image[tf.newaxis, ...])) mpe = tf.keras.backend.eval(mp) display([sample_image, sample_mask, mpe]) . show_predictions() . Let&#39;s observe how the model improves while it is training. To accomplish this task, a callback function is defined below to plot a validation image and it&#39;s predicted mask after each epoch. . class DisplayCallback(tf.keras.callbacks.Callback): def on_epoch_end(self, epoch, logs=None): clear_output(wait=True) show_predictions() print (&#39; nSample Prediction after epoch {} n&#39;.format(epoch+1)) . Now we will actually train the model for 20 epochs (full sycles through the training dataset), visualizing predictions on a validation image after each epoch. . EPOCHS = 20 model_history = model.fit(train_ds, steps_per_epoch=int(np.ceil(num_train_examples / float(batch_size))), epochs=EPOCHS, validation_data=val_ds, validation_steps=int(np.ceil(num_val_examples / float(batch_size))), callbacks=[DisplayCallback()]) . Sample Prediction after epoch 20 604/604 [==============================] - 376s 623ms/step - loss: 0.0508 - accuracy: 0.9732 - val_loss: 0.0726 - val_accuracy: 0.9653 . Plot the model&#39;s learning curve over time. . loss = model_history.history[&#39;loss&#39;] val_loss = model_history.history[&#39;val_loss&#39;] epochs = range(EPOCHS) plt.figure() plt.plot(epochs, loss, &#39;r&#39;, label=&#39;Training loss&#39;) plt.plot(epochs, val_loss, &#39;bo&#39;, label=&#39;Validation loss&#39;) plt.title(&#39;Training and Validation Loss&#39;) plt.xlabel(&#39;Epoch&#39;) plt.ylabel(&#39;Loss Value&#39;) plt.ylim([0, 1]) plt.legend() plt.show() . . Make predictions . Let&#39;s make some predictions. In the interest of saving time, the number of epochs was kept small, but you may set this higher to achieve more accurate results. . def get_predictions(dataset=None, num=1): if dataset: for image, mask in dataset.take(num): pred_mask = model.predict(image) return pred_mask else: pred_mask = create_mask(model.predict(sample_image[tf.newaxis, ...])) pred_mask = tf.keras.backend.eval(pred_mask) return pred_mask . Single image example . display_num = 1 r_choices = np.random.choice(num_foreground_examples, 1) for i in range(0, display_num * 2, 2): img_num = r_choices[i // 2] temp_ds = get_baseline_dataset(foreground_list_x[img_num:img_num+1], foreground_list_y[img_num:img_num+1], preproc_fn=tr_preprocessing_fn, batch_size=1, shuffle=False) # Let&#39;s examine some of these augmented images iterator = iter(temp_ds) next_element = iterator.get_next() batch_of_imgs, label = next_element # Running next element in our graph will produce a batch of images sample_image, sample_mask = batch_of_imgs[0], label[0,:,:,:] # run and plot predicitions pred_mask = get_predictions() show_predictions() . Multi image example . tiled_prediction_dir = os.path.join(root_dir,&#39;tiled/predictions/&#39;) if not os.path.exists(tiled_prediction_dir): os.makedirs(tiled_prediction_dir) pred_masks = [] true_masks = [] for i in range(0, num_foreground_examples): img_num = i temp_ds = get_baseline_dataset(foreground_list_x[img_num:img_num+1], foreground_list_y[img_num:img_num+1], preproc_fn=tr_preprocessing_fn, batch_size=1, shuffle=False) # Let&#39;s examine some of these augmented images iterator = iter(temp_ds) next_element = iterator.get_next() batch_of_imgs, label = next_element # Running next element in our graph will produce a batch of images sample_image, sample_mask = batch_of_imgs[0], label[0,:,:,:] true_masks.append(sample_mask) # run and plot predicitions show_predictions() pred_mask = get_predictions() pred_masks.append(pred_mask) # save prediction images to file filename_split = os.path.splitext(foreground_list_x[img_num]) filename_zero, fileext = filename_split basename = os.path.basename(filename_zero) tf.keras.preprocessing.image.save_img(tiled_prediction_dir+&#39;/&#39;+basename+&quot;.png&quot;,pred_mask) . Compute confusion matrix from all predicted images and their ground truth label masks. . # flatten our tensors and use scikit-learn to create a confusion matrix flat_preds = tf.reshape(pred_masks, [-1]) flat_truth = tf.reshape(true_masks, [-1]) cm = confusion_matrix(flat_truth, flat_preds, labels=list(range(OUTPUT_CHANNELS))) . # check values in predicted masks vs truth masks check_preds = tf.keras.backend.eval(flat_preds) check_truths = tf.keras.backend.eval(flat_truth) print(np.unique(check_preds), np.unique(check_truths)) . [0 1 3 4 5 6 7 8 9] [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.] . classes = [0,1,2,3,4,5,6,7,8,9] %matplotlib inline cm = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis] fig, ax = plt.subplots(figsize=(10, 10)) im = ax.imshow(cm, interpolation=&#39;nearest&#39;, cmap=plt.cm.Blues) ax.figure.colorbar(im, ax=ax) # We want to show all ticks... ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), # ... and label them with the respective list entries xticklabels=list(range(OUTPUT_CHANNELS)), yticklabels=list(range(OUTPUT_CHANNELS)), title=&#39;Normalized Confusion Matrix&#39;, ylabel=&#39;True label&#39;, xlabel=&#39;Predicted label&#39;) # Rotate the tick labels and set their alignment. plt.setp(ax.get_xticklabels(), rotation=45, ha=&quot;right&quot;, rotation_mode=&quot;anchor&quot;) # Loop over data dimensions and create text annotations. fmt = &#39;.2f&#39; #&#39;d&#39; # if normalize else &#39;d&#39; thresh = cm.max() / 2. for i in range(cm.shape[0]): for j in range(cm.shape[1]): ax.text(j, i, format(cm[i, j], fmt), ha=&quot;center&quot;, va=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) fig.tight_layout(pad=2.0, h_pad=2.0, w_pad=2.0) ax.set_ylim(len(classes)-0.5, -0.5) . (9.5, -0.5) . Save model to file . We will export the final model weights. . save_model_path = os.path.join(root_dir,&#39;model_out/&#39;) !mkdir $save_model_path model.save(save_model_path) . WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version. Instructions for updating: This property should not be used in TensorFlow 2.0, as updates are applied automatically. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating: This property should not be used in TensorFlow 2.0, as updates are applied automatically. INFO:tensorflow:Assets written to: ./model_out/assets . Reference . This tutorial was adapted from TensorFlow Developers tutorial on segmentation .",
            "url": "https://developmentseed.github.io/sat-ml-training/DeepLearning_CropType_Segmentation",
            "relUrl": "/DeepLearning_CropType_Segmentation",
            "date": " • Feb 24, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Random Forest Model for Crop Type and Land Classification",
            "content": "Using RandomForest Classifier for crop type mapping for SERVIR Sat ML training. This notebook showcase how we can read satellite imagery (Sentinal-2) from AWS public S3 through AWS Public Data Program for crop type mapping. . If you want to run this whole notebook on Google Colab, here is the link. . | If you would like to replicate the workflow with the same data on your local machine, please download the data from our shared Google Drive folder. . | . Besides mapping crop type with RandomForestClassifier, we also prepared notebooks that use LightGBM and SVM. . Find our notebook for LightGBM on Google Colab; | Find our notebook for SVM on Google Colab. | . from os import path as op import pickle import geopandas as gpd import matplotlib.pyplot as plt import numpy as np import rasterio from rasterio.features import rasterize from rasterstats.io import bounds_window import rasterstats from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from treeinterpreter import treeinterpreter as ti . Label data preparation . TODOs . Add training dataset sourcing, creations . # read in training data polygons that created as geojson training_vectors = gpd.read_file(&#39;training_data.geojson&#39;) training_vectors.head() . name description geometry . 0 Shadow | None | MULTIPOLYGON (((34.83383 1.18204, 34.83577 1.1... | . 1 Forestland | None | MULTIPOLYGON (((35.30961 1.01328, 35.30964 1.0... | . 2 Maize | early reproductive | MULTIPOLYGON (((34.90904 1.09515, 34.90907 1.0... | . 3 Sugarcane | no change..maize farm on the right and far lef... | MULTIPOLYGON (((34.90750 1.08934, 34.90753 1.0... | . 4 Maize | reproductive good crop | MULTIPOLYGON (((34.87144 0.82953, 34.87147 0.8... | . # find all unique values of training data names to use as classes classes = np.unique(training_vectors.name) classes . array([&#39;Built&#39;, &#39;Cloud&#39;, &#39;Fallow&#39;, &#39;Forestland&#39;, &#39;Grassland&#39;, &#39;Maize&#39;, &#39;Shadow&#39;, &#39;Sugarcane&#39;, &#39;Sunflower&#39;, &#39;Waterbody&#39;], dtype=object) . # create a dictionary to convert class names into integers for modeling class_dict = dict(zip(classes, range(len(classes)))) class_dict . {&#39;Built&#39;: 0, &#39;Cloud&#39;: 1, &#39;Fallow&#39;: 2, &#39;Forestland&#39;: 3, &#39;Grassland&#39;: 4, &#39;Maize&#39;: 5, &#39;Shadow&#39;: 6, &#39;Sugarcane&#39;: 7, &#39;Sunflower&#39;: 8, &#39;Waterbody&#39;: 9} . Reading COG from AWS . TODOs How to read Sentinel-2 from AWS public S3. . # raster information raster_file = &#39;Trans_nzoia_2019_05-02.tif&#39; . Model training . # a custom function for getting each value from the raster def all_values(x): return x # this larger cell reads data from a raster file for each training vector X_raw = [] y_raw = [] with rasterio.open(raster_file, &#39;r&#39;) as src: for (label, geom) in zip(training_vectors.name, training_vectors.geometry): # read the raster data matching the geometry bounds window = bounds_window(geom.bounds, src.transform) # store our window information window_affine = src.window_transform(window) fsrc = src.read(window=window) # rasterize the geometry into the larger shape and affine mask = rasterize( [(geom, 1)], out_shape=fsrc.shape[1:], transform=window_affine, fill=0, dtype=&#39;uint8&#39;, all_touched=True ).astype(bool) # for each label pixel (places where the mask is true) label_pixels = np.argwhere(mask) for (row, col) in label_pixels: # add a pixel of data to X data = fsrc[:,row,col] one_x = np.nan_to_num(data, nan=1e-3) X_raw.append(one_x) # add the label to y y_raw.append(class_dict[label]) . # convert the training data lists into the appropriate numpy array shape and format for scikit-learn X = np.array(X_raw) y = np.array(y_raw) (X.shape, y.shape) . ((160461, 6), (160461,)) . # helper function for calculating ND*I indices (bands in the final dimension) def band_index(arr, a, b): return np.expand_dims((arr[..., a] - arr[..., b]) / (arr[..., a] + arr[..., b]), axis=1) ndvi = band_index(X, 3, 2) ndwi = band_index(X, 1, 3) X = np.concatenate([X, ndvi, ndwi], axis=1) X.shape . (160461, 8) . # split the data into test and train sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) . # calculate class weights to allow for training on inbalanced training samples labels, counts = np.unique(y_train, return_counts=True) class_weight_dict = dict(zip(labels, 1 / counts)) class_weight_dict . {0: 0.00046882325363338024, 1: 0.001597444089456869, 2: 0.0004928536224741252, 3: 1.970093973482535e-05, 4: 0.000819000819000819, 5: 1.5704257424187697e-05, 6: 0.0002473410833539451, 7: 0.0002824858757062147, 8: 0.05263157894736842, 9: 0.003115264797507788} . # initialize a RandomForestClassifier clf = RandomForestClassifier( n_estimators=200, class_weight=class_weight_dict, max_depth=6, n_jobs=-1, verbose=1, random_state=0) . # fit the model to the data (training) clf.fit(X, y) . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers. [Parallel(n_jobs=-1)]: Done 46 tasks | elapsed: 5.3s [Parallel(n_jobs=-1)]: Done 196 tasks | elapsed: 22.1s [Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 22.6s finished . RandomForestClassifier(bootstrap=True, class_weight={0: 0.00046882325363338024, 1: 0.001597444089456869, 2: 0.0004928536224741252, 3: 1.970093973482535e-05, 4: 0.000819000819000819, 5: 1.5704257424187697e-05, 6: 0.0002473410833539451, 7: 0.0002824858757062147, 8: 0.05263157894736842, 9: 0.003115264797507788}, criterion=&#39;gini&#39;, max_depth=6, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1, oob_score=False, random_state=0, verbose=1, warm_start=False) . # predict on X_test to evaluate the model preds = clf.predict(X_test) cm = confusion_matrix(y_test, preds, labels=labels) . [Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers. [Parallel(n_jobs=2)]: Done 46 tasks | elapsed: 0.1s [Parallel(n_jobs=2)]: Done 196 tasks | elapsed: 0.5s [Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed: 0.5s finished . # (optional) save the trained model as pickle file model_name = &#39;random_forest.sav&#39; with open(model_name, &#39;wb&#39;) as modelfile: pickle.dump(clf, modelfile) . # plot the confusion matrix %matplotlib inline cm = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis] fig, ax = plt.subplots(figsize=(10, 10)) im = ax.imshow(cm, interpolation=&#39;nearest&#39;, cmap=plt.cm.Blues) ax.figure.colorbar(im, ax=ax) # We want to show all ticks... ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), # ... and label them with the respective list entries xticklabels=classes, yticklabels=classes, title=&#39;Normalized Confusion Matrix&#39;, ylabel=&#39;True label&#39;, xlabel=&#39;Predicted label&#39;) # Rotate the tick labels and set their alignment. plt.setp(ax.get_xticklabels(), rotation=45, ha=&quot;right&quot;, rotation_mode=&quot;anchor&quot;) # Loop over data dimensions and create text annotations. fmt = &#39;.2f&#39; thresh = cm.max() / 2. for i in range(cm.shape[0]): for j in range(cm.shape[1]): ax.text(j, i, format(cm[i, j], fmt), ha=&quot;center&quot;, va=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) fig.tight_layout() . # predict again with the tree interpreter to see how much each band contributes to the classification sample = 100 prediction, bias, contributions = ti.predict(clf, X_test[:sample]) c = np.sum(contributions, axis=0) . # plot the contributions band_names = [&#39;Blue&#39;, &#39;Green&#39;, &#39;Red&#39;, &#39;NIR&#39;, &#39;SWIR1&#39;, &#39;SWIR2&#39;, &#39;NDVI&#39;, &#39;NDWI&#39;] gdf = gpd.GeoDataFrame(c, columns=classes, index=band_names) gdf.style.background_gradient(cmap=&#39;viridis&#39;) . Built Cloud Fallow Forestland Grassland Maize Shadow Sugarcane Sunflower Waterbody . Blue -0.708147 | -0.450384 | -2.68955 | 5.93931 | -0.460897 | 0.36023 | -0.154207 | -0.481766 | -0.167871 | -1.18672 | . Green -1.09678 | -0.00350109 | -1.57525 | 2.95823 | -0.336196 | 1.36762 | 0.551725 | -0.218106 | -0.732232 | -0.915498 | . Red -1.90573 | -0.59379 | 0.277761 | 3.01317 | 0.223731 | 0.7112 | -0.249443 | -0.619405 | -0.683663 | -0.173831 | . NIR 0.0684765 | -0.196796 | -2.96226 | 2.66494 | -0.747003 | 1.36166 | 0.791034 | 0.479715 | -1.19875 | -0.261016 | . SWIR1 0.166388 | 0.0139419 | -1.05339 | 3.05759 | -0.538389 | 0.81744 | 0.452637 | 0.357217 | -0.771856 | -2.50158 | . SWIR2 -0.734541 | -0.460321 | 0.715332 | 2.85215 | -0.179285 | 0.235555 | -0.271136 | -0.406578 | -1.01603 | -0.735149 | . NDVI -0.919315 | -0.238332 | 2.18171 | 3.64759 | -0.992437 | 1.17432 | -0.263332 | -0.444098 | -2.37513 | -1.77098 | . NDWI 0.121986 | 0.0502156 | 0.987004 | 2.8981 | -1.07648 | 0.740517 | 0.576891 | -0.444139 | -2.01388 | -1.84022 | . Generate predictions over the full image . Using the trained RandomForestClassifier clf over a new satellite image that cover a larger geospatial location. . # in this case, we predict over the entire input image # (only small portions were used for training) new_image = raster_file output_image = &quot;classification.tif&quot; with rasterio.open(new_image, &#39;r&#39;) as src: profile = src.profile profile.update( dtype=rasterio.uint8, count=1, ) with rasterio.open(output_image, &#39;w&#39;, **profile) as dst: # perform prediction on each small image patch to minimize required memory patch_size = 500 for i in range((src.shape[0] // patch_size) + 1): for j in range((src.shape[1] // patch_size) + 1): # define the pixels to read (and write) with rasterio windows reading window = rasterio.windows.Window( j * patch_size, i * patch_size, # don&#39;t read past the image bounds min(patch_size, src.shape[1] - j * patch_size), min(patch_size, src.shape[0] - i * patch_size)) # read the image into the proper format data = src.read(window=window) # adding indices if necessary img_swp = np.moveaxis(data, 0, 2) img_flat = img_swp.reshape(-1, img_swp.shape[-1]) img_ndvi = norm_inds(img_flat, 3, 2) img_ndwi = norm_inds(img_flat, 1, 3) img_w_ind = np.concatenate([img_flat, img_ndvi, img_ndwi], axis=1) # remove no data values, store the indices for later use m = np.ma.masked_invalid(img_w_ind) to_predict = img_w_ind[~m.mask].reshape(-1, img_w_ind.shape[-1]) # skip empty inputs if not len(to_predict): continue # predict img_preds = clf.predict(to_predict) # add the prediction back to the valid pixels (using only the first band of the mask to decide on validity) # makes the assumption that all bands have identical no-data value arrangements output = np.zeros(img_flat.shape[0]) output[~m.mask[:, 0]] = img_preds.flatten() # resize to the original image dimensions output = output.reshape(*img_swp.shape[:-1]) # create our final mask mask = (~m.mask[:, 0]).reshape(*img_swp.shape[:-1]) # write to the final files dst.write(output.astype(rasterio.uint8), 1, window=window) dst.write_mask(mask, window=window) .",
            "url": "https://developmentseed.github.io/sat-ml-training/Randomforest_cropmapping_with_COGs",
            "relUrl": "/Randomforest_cropmapping_with_COGs",
            "date": " • Feb 23, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Machine learning (ML)",
            "content": "How to navigate this guide . The order below should navigate the reader progressively through the topics that we will cover in the tutorials. The first section (1.2.0) is intended to build an understanding of machine learning fundamentals which will inform the following sections. Read the main links to articles, view/read but no need to try the code implementation examples, and for further learning - please see the references listed at the end. . 1.2.0 General Machine Learning Best Practices . What is Machine Learning? | About Train, Validation and Test Sets in Machine Learning | Handling imbalanced datasets in machine learning | Model Optimization read Abstract and section II (first two paragraphs) | Parallel and Distributed Deep Learning read sections 1.2, 2, 3 | Metrics to Evaluate your Machine Learning Algorithm | . 1.2.1 Classic Machine Learning (ML) . RandomForest . Random Forest Simple Explanation read sections: (1) Decision Tree: The Building Block and (2) From Decision Tree to Random Forest | Decision tree and RandomForest classifier in-depth for generic coding implementations . | w/ sciki-learn for the specific function . | RandomForest classifier in LULC case (Development Seed example) | . LightGBM . LightGBM: A Highly Efficient Gradient Boosting Decision Tree read Introduction and section 2.1 | LightGBM python library this is the library that we will use in the tutorial . | LightGBM in crop type mapping case (Development Seed example and SentinelHub eo-learn example) . | . 1.2.2 Deep learning . Deep Learning vs Classical Machine Learning . Deep Learning in a Nutshell – what it is, how it works, why care? read sections (1) What is Machine Learning? and (2) A First Look at Neural Networks | . Semantic Segmentation . Semantic Segmentation with Deep Learning | . TF dynamic UNet . UNet read sections 1 and 2 | Dynamic UNet in LULC case (Development Seed example) | . References: . Introduction to Statistical Learning book and examples written in python notebooks | Additional Examples from Python for Data Science | .",
            "url": "https://developmentseed.github.io/sat-ml-training/IntroMachineLearning",
            "relUrl": "/IntroMachineLearning",
            "date": " • Feb 23, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Random Forest Model for Crop Type and Land Classification",
            "content": "This notebook teaches you how to read satellite imagery (Sentinal-2) from Google Earth Engine and use it for crop type mapping with a RandomForest Classifier. . If you want to run this whole notebook on Google Colab, here is the link. . | If you would like to replicate the workflow with the same data on your local machine, please download the data from our shared Google Drive folder. . | . Besides mapping crop type with RandomForestClassifier, we also prepared notebooks that use LightGBM and SVM. . Find our notebook for LightGBM on Google Colab; | Find our notebook for SVM on Google Colab. | . Setup Notebook . # Requirements, will skip if already installed %pip install geopandas rasterio rasterstats %pip install scikit-learn %pip install treeinterpreter . from os import path as op import pickle import geopandas as gpd import matplotlib.pyplot as plt import numpy as np import rasterio from rasterio.features import rasterize from rasterstats.io import bounds_window import rasterstats from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from treeinterpreter import treeinterpreter as ti . Connect Google Drive . from google.colab import drive drive.mount(&#39;/content/drive/&#39;) . Mounted at /content/drive/ . Label data preparation . TODOs . Add training dataset sourcing, creations . # read in training data polygons that created as geojson training_data = &#39;/content/drive/Shared drives/servir-sat-ml/data/training_data.geojson&#39; training_vectors = gpd.read_file(training_data) training_vectors.head() . name description geometry . 0 Shadow | None | MULTIPOLYGON (((34.83383 1.18204, 34.83577 1.1... | . 1 Forestland | None | MULTIPOLYGON (((35.30961 1.01328, 35.30964 1.0... | . 2 Maize | early reproductive | MULTIPOLYGON (((34.90904 1.09515, 34.90907 1.0... | . 3 Sugarcane | no change..maize farm on the right and far lef... | MULTIPOLYGON (((34.90750 1.08934, 34.90753 1.0... | . 4 Maize | reproductive good crop | MULTIPOLYGON (((34.87144 0.82953, 34.87147 0.8... | . Reading image from GEE . Connect to GEE . # If not on Colab you&#39;ll need install the earth-engine Python API #!pip install earthengine-api #earth-engine Python API # Athenticate to your GEE account. !earthengine authenticate . # Earth Engine Python API import ee ee.Initialize() . Search GEE Data . # From GEE #training_vectors.total_bounds.tolist() aoi = ee.Geometry.Rectangle(training_vectors.total_bounds.tolist()) band_sel = (&#39;B2&#39;, &#39;B3&#39;, &#39;B4&#39;, &#39;B8&#39;, &#39;B11&#39;, &#39;B12&#39;) sentinel_scenes = ee.ImageCollection(&quot;COPERNICUS/S2&quot;) .filterBounds(aoi) .filterDate(&#39;2019-05-02&#39;, &#39;2019-05-03&#39;) .select(band_sel) scenes = sentinel_scenes.getInfo() [print(scene[&#39;id&#39;]) for scene in scenes[&quot;features&quot;]] sentinel_mosaic = sentinel_scenes.mean().rename(band_sel) sentinel_mosaic.getInfo() . COPERNICUS/S2/20190502T074621_20190502T080204_T36NXF COPERNICUS/S2/20190502T074621_20190502T080204_T36NXG COPERNICUS/S2/20190502T074621_20190502T080204_T36NYF COPERNICUS/S2/20190502T074621_20190502T080204_T36NYG . {&#39;bands&#39;: [{&#39;crs&#39;: &#39;EPSG:4326&#39;, &#39;crs_transform&#39;: [1, 0, 0, 0, 1, 0], &#39;data_type&#39;: {&#39;max&#39;: 65535, &#39;min&#39;: 0, &#39;precision&#39;: &#39;double&#39;, &#39;type&#39;: &#39;PixelType&#39;}, &#39;id&#39;: &#39;B2&#39;}, {&#39;crs&#39;: &#39;EPSG:4326&#39;, &#39;crs_transform&#39;: [1, 0, 0, 0, 1, 0], &#39;data_type&#39;: {&#39;max&#39;: 65535, &#39;min&#39;: 0, &#39;precision&#39;: &#39;double&#39;, &#39;type&#39;: &#39;PixelType&#39;}, &#39;id&#39;: &#39;B3&#39;}, {&#39;crs&#39;: &#39;EPSG:4326&#39;, &#39;crs_transform&#39;: [1, 0, 0, 0, 1, 0], &#39;data_type&#39;: {&#39;max&#39;: 65535, &#39;min&#39;: 0, &#39;precision&#39;: &#39;double&#39;, &#39;type&#39;: &#39;PixelType&#39;}, &#39;id&#39;: &#39;B4&#39;}, {&#39;crs&#39;: &#39;EPSG:4326&#39;, &#39;crs_transform&#39;: [1, 0, 0, 0, 1, 0], &#39;data_type&#39;: {&#39;max&#39;: 65535, &#39;min&#39;: 0, &#39;precision&#39;: &#39;double&#39;, &#39;type&#39;: &#39;PixelType&#39;}, &#39;id&#39;: &#39;B8&#39;}, {&#39;crs&#39;: &#39;EPSG:4326&#39;, &#39;crs_transform&#39;: [1, 0, 0, 0, 1, 0], &#39;data_type&#39;: {&#39;max&#39;: 65535, &#39;min&#39;: 0, &#39;precision&#39;: &#39;double&#39;, &#39;type&#39;: &#39;PixelType&#39;}, &#39;id&#39;: &#39;B11&#39;}, {&#39;crs&#39;: &#39;EPSG:4326&#39;, &#39;crs_transform&#39;: [1, 0, 0, 0, 1, 0], &#39;data_type&#39;: {&#39;max&#39;: 65535, &#39;min&#39;: 0, &#39;precision&#39;: &#39;double&#39;, &#39;type&#39;: &#39;PixelType&#39;}, &#39;id&#39;: &#39;B12&#39;}], &#39;type&#39;: &#39;Image&#39;} . Exporting Image . # We will save it to Google Drive for later reuse output_mount_folder = &#39;/content/drive/My Drive&#39; output_drive_folder = &#39;Colab Notebooks/data&#39; raster_name = &#39;sentinel_mosaic-Trans_Nzoia&#39; . # If you want to keep track of the export import time while task.active(): print(&#39;Polling for task (id: {}).&#39;.format(task.id)) time.sleep(15) . # Reference the raster on disk. raster_path = op.join(output_mount_folder, output_drive_folder, raster_name) raster_file = &quot;.&quot;.join([raster_path, &quot;tif&quot;]) # Alternate reference already prepared file on Google Drive, uncomment next line to use #raster_file = &#39;/content/drive/Shared drives/servir-sat-ml/data/Trans_nzoia_2019_05-02.tif&#39; print(raster_file) . Model training . Prepare Data . # find all unique values of training data names to use as classes classes = np.unique(training_vectors.name) classes . array([&#39;Built&#39;, &#39;Cloud&#39;, &#39;Fallow&#39;, &#39;Forestland&#39;, &#39;Grassland&#39;, &#39;Maize&#39;, &#39;Shadow&#39;, &#39;Sugarcane&#39;, &#39;Sunflower&#39;, &#39;Waterbody&#39;], dtype=object) . # create a dictionary to convert class names into integers for modeling class_dict = dict(zip(classes, range(len(classes)))) class_dict . {&#39;Built&#39;: 0, &#39;Cloud&#39;: 1, &#39;Fallow&#39;: 2, &#39;Forestland&#39;: 3, &#39;Grassland&#39;: 4, &#39;Maize&#39;: 5, &#39;Shadow&#39;: 6, &#39;Sugarcane&#39;: 7, &#39;Sunflower&#39;: 8, &#39;Waterbody&#39;: 9} . # raster information # a custom function for getting each value from the raster def all_values(x): return x # this larger cell reads data from a raster file for each training vector X_raw = [] y_raw = [] with rasterio.open(raster_file, &#39;r&#39;) as src: for (label, geom) in zip(training_vectors.name, training_vectors.geometry): # read the raster data matching the geometry bounds window = bounds_window(geom.bounds, src.transform) # store our window information window_affine = src.window_transform(window) fsrc = src.read(window=window) # rasterize the geometry into the larger shape and affine mask = rasterize( [(geom, 1)], out_shape=fsrc.shape[1:], transform=window_affine, fill=0, dtype=&#39;uint8&#39;, all_touched=True ).astype(bool) # for each label pixel (places where the mask is true) label_pixels = np.argwhere(mask) for (row, col) in label_pixels: # add a pixel of data to X data = fsrc[:,row,col] one_x = np.nan_to_num(data, nan=1e-3) X_raw.append(one_x) # add the label to y y_raw.append(class_dict[label]) . # convert the training data lists into the appropriate numpy array shape and format for scikit-learn X = np.array(X_raw) y = np.array(y_raw) (X.shape, y.shape) . ((160461, 6), (160461,)) . # helper function for calculating ND*I indices (bands in the final dimension) def band_index(arr, a, b): return np.expand_dims((arr[..., a] - arr[..., b]) / (arr[..., a] + arr[..., b]), axis=1) ndvi = band_index(X, 3, 2) ndwi = band_index(X, 1, 3) X = np.concatenate([X, ndvi, ndwi], axis=1) X.shape . (160461, 8) . # split the data into test and train sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) . # calculate class weights to allow for training on inbalanced training samples labels, counts = np.unique(y_train, return_counts=True) class_weight_dict = dict(zip(labels, 1 / counts)) class_weight_dict . {0: 0.00046882325363338024, 1: 0.001597444089456869, 2: 0.0004928536224741252, 3: 1.970093973482535e-05, 4: 0.000819000819000819, 5: 1.5704257424187697e-05, 6: 0.0002473410833539451, 7: 0.0002824858757062147, 8: 0.05263157894736842, 9: 0.003115264797507788} . Train RandomForest . # initialize a RandomForestClassifier clf = RandomForestClassifier( n_estimators=200, class_weight=class_weight_dict, max_depth=6, n_jobs=-1, verbose=1, random_state=0) . # fit the model to the data (training) clf.fit(X, y) . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers. [Parallel(n_jobs=-1)]: Done 46 tasks | elapsed: 6.6s [Parallel(n_jobs=-1)]: Done 196 tasks | elapsed: 27.0s [Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 27.5s finished . RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={0: 0.00046882325363338024, 1: 0.001597444089456869, 2: 0.0004928536224741252, 3: 1.970093973482535e-05, 4: 0.000819000819000819, 5: 1.5704257424187697e-05, 6: 0.0002473410833539451, 7: 0.0002824858757062147, 8: 0.05263157894736842, 9: 0.003115264797507788}, criterion=&#39;gini&#39;, max_depth=6, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1, oob_score=False, random_state=0, verbose=1, warm_start=False) . # predict on X_test to evaluate the model preds = clf.predict(X_test) cm = confusion_matrix(y_test, preds, labels=labels) . [Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers. [Parallel(n_jobs=2)]: Done 46 tasks | elapsed: 0.2s [Parallel(n_jobs=2)]: Done 196 tasks | elapsed: 0.6s [Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed: 0.6s finished . # (optional) save the trained model as pickle file model_name = &#39;/content/drive/My Drive/Colab Notebooks/data/random_forest.sav&#39; with open(model_name, &#39;wb&#39;) as modelfile: pickle.dump(clf, modelfile) . Assess the Model . # plot the confusion matrix %matplotlib inline cm = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis] fig, ax = plt.subplots(figsize=(10, 10)) im = ax.imshow(cm, interpolation=&#39;nearest&#39;, cmap=plt.cm.Blues) ax.figure.colorbar(im, ax=ax) # We want to show all ticks... ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), # ... and label them with the respective list entries xticklabels=classes, yticklabels=classes, title=&#39;Normalized Confusion Matrix&#39;, ylabel=&#39;True label&#39;, xlabel=&#39;Predicted label&#39;) # Rotate the tick labels and set their alignment. plt.setp(ax.get_xticklabels(), rotation=45, ha=&quot;right&quot;, rotation_mode=&quot;anchor&quot;) # Loop over data dimensions and create text annotations. fmt = &#39;.2f&#39; thresh = cm.max() / 2. for i in range(cm.shape[0]): for j in range(cm.shape[1]): ax.text(j, i, format(cm[i, j], fmt), ha=&quot;center&quot;, va=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) fig.tight_layout() . # predict again with the tree interpreter to see how much each band contributes to the classification sample = 100 prediction, bias, contributions = ti.predict(clf, X_test[:sample]) c = np.sum(contributions, axis=0) . # plot the contributions band_names = [&#39;Blue&#39;, &#39;Green&#39;, &#39;Red&#39;, &#39;NIR&#39;, &#39;SWIR1&#39;, &#39;SWIR2&#39;, &#39;NDVI&#39;, &#39;NDWI&#39;] gdf = gpd.GeoDataFrame(c, columns=classes, index=band_names) gdf.style.background_gradient(cmap=&#39;viridis&#39;) . Built Cloud Fallow Forestland Grassland Maize Shadow Sugarcane Sunflower Waterbody . Blue -0.708147 | -0.450384 | -2.689550 | 5.939310 | -0.460897 | 0.360230 | -0.154207 | -0.481766 | -0.167871 | -1.186718 | . Green -1.096782 | -0.003501 | -1.575254 | 2.958226 | -0.336196 | 1.367617 | 0.551725 | -0.218106 | -0.732232 | -0.915498 | . Red -1.905729 | -0.593790 | 0.277761 | 3.013168 | 0.223731 | 0.711200 | -0.249443 | -0.619405 | -0.683663 | -0.173831 | . NIR 0.068476 | -0.196796 | -2.962262 | 2.664941 | -0.747003 | 1.361658 | 0.791034 | 0.479715 | -1.198747 | -0.261016 | . SWIR1 0.166388 | 0.013942 | -1.053387 | 3.057588 | -0.538389 | 0.817440 | 0.452637 | 0.357217 | -0.771856 | -2.501579 | . SWIR2 -0.734541 | -0.460321 | 0.715332 | 2.852150 | -0.179285 | 0.235555 | -0.271136 | -0.406578 | -1.016028 | -0.735149 | . NDVI -0.919315 | -0.238332 | 2.181709 | 3.647589 | -0.992437 | 1.174317 | -0.263332 | -0.444098 | -2.375126 | -1.770976 | . NDWI 0.121986 | 0.050216 | 0.987004 | 2.898101 | -1.076480 | 0.740517 | 0.576891 | -0.444139 | -2.013875 | -1.840221 | . Generate predictions over the full image . Using the trained RandomForestClassifier clf over a new satellite image that cover a larger geospatial location. . # in this case, we predict over the entire input image # (only small portions were used for training) new_image = raster_file output_image = &quot;/content/drive/My Drive/Colab Notebooks/data/classification.tif&quot; with rasterio.open(new_image, &#39;r&#39;) as src: profile = src.profile profile.update( dtype=rasterio.uint8, count=1, ) with rasterio.open(output_image, &#39;w&#39;, **profile) as dst: # perform prediction on each small image patch to minimize required memory patch_size = 500 for i in range((src.shape[0] // patch_size) + 1): for j in range((src.shape[1] // patch_size) + 1): # define the pixels to read (and write) with rasterio windows reading window = rasterio.windows.Window( j * patch_size, i * patch_size, # don&#39;t read past the image bounds min(patch_size, src.shape[1] - j * patch_size), min(patch_size, src.shape[0] - i * patch_size)) # read the image into the proper format data = src.read(window=window) # adding indices if necessary img_swp = np.moveaxis(data, 0, 2) img_flat = img_swp.reshape(-1, img_swp.shape[-1]) img_ndvi = band_index(img_flat, 3, 2) img_ndwi = band_index(img_flat, 1, 3) img_w_ind = np.concatenate([img_flat, img_ndvi, img_ndwi], axis=1) # remove no data values, store the indices for later use m = np.ma.masked_invalid(img_w_ind) to_predict = img_w_ind[~m.mask].reshape(-1, img_w_ind.shape[-1]) # skip empty inputs if not len(to_predict): continue # predict img_preds = clf.predict(to_predict) # add the prediction back to the valid pixels (using only the first band of the mask to decide on validity) # makes the assumption that all bands have identical no-data value arrangements output = np.zeros(img_flat.shape[0]) output[~m.mask[:, 0]] = img_preds.flatten() # resize to the original image dimensions output = output.reshape(*img_swp.shape[:-1]) # create our final mask mask = (~m.mask[:, 0]).reshape(*img_swp.shape[:-1]) # write to the final files dst.write(output.astype(rasterio.uint8), 1, window=window) dst.write_mask(mask, window=window) .",
            "url": "https://developmentseed.github.io/sat-ml-training/Randomforest_cropmapping-with_GEE",
            "relUrl": "/Randomforest_cropmapping-with_GEE",
            "date": " • Feb 22, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Geospatial Python",
            "content": "The following material covers the basics of using spatial data in python. The main goal is to become familiar with the libraries used, and to try a few examples of operations with vector, and raster data, including some basic visualizations. . Vector Data . . Note: A GeoDataFrame is a pandas DataFrame with geometries (GeoSeries) . How to load and save spatial data with Geopandas | General Data Manipulation (Geopandas) | Subsetting by Attributes, to select records based on attributes use the techniques from Pandas | Projections | Intersects | Spatial Join | Spatial Aggregation | Derive Centroids | Bounding Box For each row in a GeoDataFrame GeoSeries.bounds if you want to extract the coordinates. | For each row in a GeoDataFrame if you want another geodataframe you can do spatial operations with GeoSeries.envelope | For a whole GeoDataFrame GeoSeries.total_bounds | . | . import geopandas url = &quot;https://d2ad6b4ur7yvpq.cloudfront.net/naturalearth-3.3.0/ne_110m_admin_0_countries.geojson&quot; countries_gdf = geopandas.read_file(url) print(countries_gdf.head()) . scalerank labelrank sovereignt sov_a3 adm0_dif level 0 1 3 Afghanistan AFG 0 2 1 1 3 Angola AGO 0 2 2 1 6 Albania ALB 0 2 3 1 4 United Arab Emirates ARE 0 2 4 1 2 Argentina ARG 0 2 type admin adm0_a3 geou_dif ... region_un 0 Sovereign country Afghanistan AFG 0 ... Asia 1 Sovereign country Angola AGO 0 ... Africa 2 Sovereign country Albania ALB 0 ... Europe 3 Sovereign country United Arab Emirates ARE 0 ... Asia 4 Sovereign country Argentina ARG 0 ... Americas subregion region_wb name_len long_len abbrev_len 0 Southern Asia South Asia 11 11 4 1 Middle Africa Sub-Saharan Africa 6 6 4 2 Southern Europe Europe &amp; Central Asia 7 7 4 3 Western Asia Middle East &amp; North Africa 20 20 6 4 South America Latin America &amp; Caribbean 9 9 4 tiny homepart featureclass 0 -99 1 Admin-0 country 1 -99 1 Admin-0 country 2 -99 1 Admin-0 country 3 -99 1 Admin-0 country 4 -99 1 Admin-0 country geometry 0 POLYGON ((61.21082 35.65007, 62.23065 35.27066... 1 MULTIPOLYGON (((16.32653 -5.87747, 16.57318 -6... 2 POLYGON ((20.59025 41.85540, 20.46318 41.51509... 3 POLYGON ((51.57952 24.24550, 51.75744 24.29407... 4 MULTIPOLYGON (((-65.50000 -55.20000, -66.45000... [5 rows x 64 columns] . countries_gdf.total_bounds . [-180. -90. 180. 83.64513] . countries_gdf.head().bounds . minx miny maxx maxy 0 60.528430 29.318572 75.158028 38.486282 1 11.640096 -17.930636 24.079905 -4.438023 2 19.304486 39.624998 21.020040 42.688247 3 51.579519 22.496948 56.396847 26.055464 4 -73.415436 -55.250000 -53.628349 -21.832310 . countries_gdf.head().envelope . 0 POLYGON ((60.52843 29.31857, 75.15803 29.31857... 1 POLYGON ((11.64010 -17.93064, 24.07991 -17.930... 2 POLYGON ((19.30449 39.62500, 21.02004 39.62500... 3 POLYGON ((51.57952 22.49695, 56.39685 22.49695... 4 POLYGON ((-73.41544 -55.25000, -53.62835 -55.2... dtype: geometry . Making Maps . Static, more examples | Dynamic maps using Folium | . Optional Bonus Material . Advanced Vector Input/Output(I/O) with Fiona | Using Spatial Indexes for faster spatial operations | . Raster . How to load and save data . Rasterio (Reading) | Rasterio (Writing). The recommended default writing Profile is a Cloud Optimized Geotiff, as shown with the rio-cogeo library. | . | Numpy Arrays (Rasters) . Clipping raster by AOI | Band Math (aka Map Algebra) | Sampling data (extract) from raster with a vector | . | . Making Maps . Static | Dynamic with Vector and Raster Data in Folium | . Additional References . The majority of lessons come from . AutoGIS | EarthLab | . Each Library has it&#39;s own great documentation . Geopandas for vector geometry and attribute handling | Shapely for vector geometry operations that Geopandas doesn’t do. Geopandas actually imports Shapely for most operations. | Rasterio | rio-cogeo | folium | .",
            "url": "https://developmentseed.github.io/sat-ml-training/GeospatialPython",
            "relUrl": "/GeospatialPython",
            "date": " • Feb 22, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Intro to Python",
            "content": "General python . In this section we review the fundamentals of using the Python programming language with a focus on being able to use python for geospatial data science with machine learning. Based on the list of goals, and your prior experience select from the list of tutorials. Feel free to do as many as you like, and skim lightly if you are already familiar with the topics. . For local practice you can install and user the Python with Conda, we recommend coding in a Jupyter Notebook which is similar to the Google Colab online editor that will be used in the other tutorials on this site. . Goals . Data types . Basic Types: Integers (Whole numbers), Floats (Numbers with decimals), Strings | Constructed Types: Lists, Dictionaries . Note: Python Dictionaries and JSON files very similar. | . | Looping (Program flow control) . Using looping to iterate over a set of items to repeat a process . Example: Loop over a list of items . | Example: Loop over list of files . | . | . | Calling functions . Importing libraries (aka modules, packages) | How to call a function | How to write a small function | . | . Lessons . Pick from the following lessons. We recommend at least 1, and if you choose to watch the videos then we suggest you also do some of the Interactive coding examples from the other options. Trying some code is one of the best ways to learn. . An introduction or refresher for Python 3 Online Videos from Udemy | Interactive lessons with examples and quizzes from CodeAcademy. All lessons except #10 on Classes (optional) | Demonstration Notebooks that Accompany the Whirlwind Tour Of Python Book (Available for free). Lessons 1-9, everything else is optional. | Additional References . Python Documentation https://docs.python.org/3/tutorial/index.html . | More tutorials https://docs.python-guide.org/intro/learning/ . | . Python for Data Science . In this section we specifically review Python for Data Science, focusing on the main data structures used to manipulate data, numpy arrays and pandas dataframes. Each link is a tutorial or example that you can try. . Numpy Arrays . . Important: Understanding multidimensional data arrays as a data structure. . Note: Numpy array data must all be of the same type (e.g. Integer, Float, etc…) . Lessons: . Introduction to NumPy . | Understanding Data Types in Python . | The Basics of NumPy Arrays . | Computation on NumPy Arrays: Universal Functions . | Aggregations: Min, Max, and Everything In Between . | . Pandas DataFrames . . Important: Understanding data frames as a table (sheet), with data records as rows, and data attributes as columns. Each column can be a different data type. . Lessons: . Introduction . | Importing and exporting CSV and JSON data sources . | Summarizing (aggregating) data . | Selecting relevant data records . | All in one . | (Optional) Further Reading and Materials . | . References . Primary Source PythonDataScienceHandbook | Secondary Source Datacarpentry python-ecology-lesson | . Plotting . . Important: Being able to visualize selections of data in a variety of standard plot types. Histogram, Bar, Line, Scatter Plot (XY) . Visualization with Matplotlib . | Simple Line Plots . | Scatter Plots . | Density and Contour Plots . | Histograms, Binnings, and Density . | .",
            "url": "https://developmentseed.github.io/sat-ml-training/IntroPython",
            "relUrl": "/IntroPython",
            "date": " • Feb 21, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: main- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-2-88b922e30289&gt; in &lt;module&gt; 1 #collapse-hide 2 import pandas as pd -&gt; 3 import altair as alt ModuleNotFoundError: No module named &#39;altair&#39; . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://developmentseed.github.io/sat-ml-training/test",
            "relUrl": "/test",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "This work is licensed under a Creative Commons Attribution 4.0 International License. . It was developed by Development Seed for the SERVIR GLOBAL program. NASA &amp; USAID), with feedback from: . SERVIR Hindu Kush Himalaya | SERVIR Eastern &amp; Southern Africa | SERVIR West Africa | . . Full source code can be found on Github . This website is powered by fastpages, a blogging platform that natively supports Jupyter notebooks in addition to other formats. .",
          "url": "https://developmentseed.github.io/sat-ml-training/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://developmentseed.github.io/sat-ml-training/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}